{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = r'C:/Users/mastr/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Windows OS\n",
    "# os.environ['PROJ_LIB'] = r'/Users/mmastro/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Mac OS\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#from scipy.signal import argrelextrema                      # Find local Maxima-Minima in numpy array\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_dim(da, dim, degree):\n",
    "    # detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=degree)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    da_det = (da - fit)\n",
    "    return da_det\n",
    "\n",
    "def xr_mean_list(xr_list, esm):\n",
    "    ## Function for calculate the mean response of several simulations from different ESM\n",
    "    # xr_list: list of xarray, each representing one simulation \n",
    "    # nc_files: list of NetCDF opened with nc.Dataset(), containing data and attributes\n",
    "    # returns an xr_list_new with one element for each esm \n",
    "\n",
    "    # List of ESM names in every simulation\n",
    "    # esm = [a.source_id for a in nc_files]\n",
    "    # esm = [nc_files]#\n",
    "\n",
    "    # Assign as a new coordinate ESM name\n",
    "    xr_list = [a.assign_coords(esm=b) for a,b in zip(xr_list,esm)]\n",
    "\n",
    "    # List of unique ESM names sorted\n",
    "    esm = list(set(esm))\n",
    "    esm.sort()\n",
    "\n",
    "    # Create a list of the different esm lists\n",
    "    access = []; bcc = []; cesm = []; cmcc = []; cnrm = []; canesm = []; e3sm = []; ipsl = []; miroc = []; mpi = []; noresm = []; taiesm = []; ukesm = []\n",
    "    esm_list = [access, bcc, cesm, cmcc, cnrm, canesm, e3sm, ipsl, miroc, mpi, noresm, taiesm, ukesm]\n",
    "\n",
    "    # populate the esm_list according to xr.coords[\"esm\"]\n",
    "    for e,model in enumerate(esm):\n",
    "        for i,xarray in enumerate(xr_list):\n",
    "            esm_coord = xarray.coords[\"esm\"]\n",
    "            if esm_coord == esm[e]:\n",
    "                esm_list[e].append(xarray)\n",
    "\n",
    "    # take the mean of all the xarray belonging to the same esm\n",
    "    xr_list_new = []\n",
    "    for e in esm_list:\n",
    "        content = xr.concat(e, dim='realiz')\n",
    "        content1 = content.mean(dim = \"realiz\")\n",
    "        xr_list_new.append(content1)\n",
    "    \n",
    "    return xr_list_new\n",
    "\n",
    "## Fuction for subsetting colormap values ## \n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "## Function for creating a path, if needed ##\n",
    "def checkDir(out_path):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "def lag_linregress_3D(x, y, lagx=0, lagy=0):\n",
    "    \"\"\"\n",
    "    Input: Two xr.Datarrays of any dimensions with the first dim being time. \n",
    "    Thus the input data could be a 1D time series, or for example, have three dimensions (time,lat,lon). \n",
    "    Datasets can be provied in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    Output: Covariance, correlation, regression slope and intercept, p-value, and standard error on regression\n",
    "    between the two datasets along their aligned time dimension.  \n",
    "    Lag values can be assigned to either of the data, with lagx shifting x, and lagy shifting y, with the specified lag amount. \n",
    "    \"\"\" \n",
    "    #1. Ensure that the data are properly alinged to each other. \n",
    "    x,y = xr.align(x,y)\n",
    "    \n",
    "    #2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx!=0:\n",
    "        #If x lags y by 1, x must be shifted 1 step backwards. \n",
    "        #But as the 'zero-th' value is nonexistant, xr assigns it as invalid (nan). Hence it needs to be dropped\n",
    "        x   = x.shift(time = -lagx).dropna(dim='time')\n",
    "        #Next important step is to re-align the two datasets so that y adjusts to the changed coordinates of x\n",
    "        x,y = xr.align(x,y)\n",
    "\n",
    "    if lagy!=0:\n",
    "        y   = y.shift(time = -lagy).dropna(dim='time')\n",
    "        x,y = xr.align(x,y)\n",
    " \n",
    "    #3. Compute data length, mean and standard deviation along time axis for further use: \n",
    "    n     = x.shape[0]\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd  = x.std(axis=0)\n",
    "    ystd  = y.std(axis=0)\n",
    "    \n",
    "    #4. Compute covariance along time axis\n",
    "    cov   =  np.sum((x - xmean)*(y - ymean), axis=0)/(n)\n",
    "    \n",
    "    #5. Compute correlation along time axis\n",
    "    cor   = cov/(xstd*ystd)\n",
    "    \n",
    "    #6. Compute regression slope and intercept:\n",
    "    slope     = cov/(xstd**2)\n",
    "    intercept = ymean - xmean*slope\n",
    "    y_pred =  intercept + slope*x\n",
    "    res = y - y_pred\n",
    "\n",
    "    #7. Compute P-value and standard error\n",
    "    #Compute t-statistics\n",
    "    tstats = cor*np.sqrt(n-2)/np.sqrt(1-cor**2)\n",
    "    stderr = slope/tstats\n",
    "    \n",
    "    from scipy.stats import t\n",
    "    pval   = t.sf(tstats, n-2)*2\n",
    "    pval   = xr.DataArray(pval, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    #return cov,cor,slope,intercept,pval,stderr\n",
    "    return res\n",
    "\n",
    "def cor_3D(x, y, lagx=0, lagy=0):\n",
    "    \"\"\"\n",
    "    Input: Two xr.Datarrays of any dimensions with the first dim being time. \n",
    "    Thus the input data could be a 1D time series, or for example, have three dimensions (time,lat,lon). \n",
    "    Datasets can be provied in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    Output: Covariance, correlation, regression slope and intercept, p-value, and standard error on regression\n",
    "    between the two datasets along their aligned time dimension.  \n",
    "    Lag values can be assigned to either of the data, with lagx shifting x, and lagy shifting y, with the specified lag amount. \n",
    "    \"\"\" \n",
    "    #1. Ensure that the data are properly alinged to each other. \n",
    "    x,y = xr.align(x,y)\n",
    "    \n",
    "    #2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx!=0:\n",
    "        #If x lags y by 1, x must be shifted 1 step backwards. \n",
    "        #But as the 'zero-th' value is nonexistant, xr assigns it as invalid (nan). Hence it needs to be dropped\n",
    "        x   = x.shift(time = -lagx).dropna(dim='time')\n",
    "        #Next important step is to re-align the two datasets so that y adjusts to the changed coordinates of x\n",
    "        x,y = xr.align(x,y)\n",
    "\n",
    "    if lagy!=0:\n",
    "        y   = y.shift(time = -lagy).dropna(dim='time')\n",
    "        x,y = xr.align(x,y)\n",
    " \n",
    "    #3. Compute data length, mean and standard deviation along time axis for further use: \n",
    "    n     = x.shape[0]\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd  = x.std(axis=0)\n",
    "    ystd  = y.std(axis=0)\n",
    "    \n",
    "    #4. Compute covariance along time axis\n",
    "    cov   =  np.sum((x - xmean)*(y - ymean), axis=0)/(n)\n",
    "    \n",
    "    #5. Compute correlation along time axis\n",
    "    cor   = cov/(xstd*ystd)\n",
    "\n",
    "    #return cov,cor,slope,intercept,pval,stderr\n",
    "    return cor\n",
    "\n",
    "def xr_multipletest(p, alpha=0.05, method='fdr_bh', **multipletests_kwargs):\n",
    "    \"\"\"Apply statsmodels.stats.multitest.multipletests for multi-dimensional xr.objects.\"\"\"\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    # stack all to 1d array\n",
    "    p_stacked = p.stack(s=p.dims)\n",
    "    # mask only where not nan: https://github.com/statsmodels/statsmodels/issues/2899\n",
    "    mask = np.isfinite(p_stacked)\n",
    "    pvals_corrected = np.full(p_stacked.shape, np.nan)\n",
    "    reject = np.full(p_stacked.shape, np.nan)\n",
    "    # apply test where mask\n",
    "    reject[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[0]\n",
    "    pvals_corrected[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[1]\n",
    "\n",
    "    def unstack(reject, p_stacked):\n",
    "        \"\"\"Exchange values from p_stacked with reject (1d array) and unstack.\"\"\"\n",
    "        xreject = p_stacked.copy()\n",
    "        xreject.values = reject\n",
    "        xreject = xreject.unstack()\n",
    "        return xreject\n",
    "\n",
    "    reject = unstack(reject, p_stacked)\n",
    "    pvals_corrected = unstack(pvals_corrected, p_stacked)\n",
    "    return reject, pvals_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Path for the output files (images, etc)\n",
    "# out_path = 'C:/Users/mastr/Documents/Amazon/RESULTS/'\n",
    "out_path = 'G:/Shared drives/Amazon_ENSO_work/RESULTS/MLR/'\n",
    "\n",
    "#out_path = \"D:/Data/CMIP6/RESULTS\"\n",
    "\n",
    "# -- Create directories\n",
    "checkDir(out_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open SST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------- Open data (MODEL) ------- #####\n",
    "data_path = 'C:/Users/mastr/Documents/Amazon'\n",
    "scenario = 'historical'\n",
    "files = \"nino34\" + '_*_' + scenario + '_*'\n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/ENSO/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "nino34_hist = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/ENSO/'+files))):               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"]); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    nino34_hist.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = \"nino34\" + '_*_' + scenario + '_*' \n",
    "\n",
    "nino34_ssp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/ENSO/'+files))):               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"]); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    nino34_ssp.append(content)\n",
    "\n",
    "## Normalization\n",
    "# for i,item in enumerate(nino34_hist):\n",
    "#     nino34_hist[i] =  ((nino34_hist[i] - (nino34_hist[i].mean(dim='time'))).compute()/(nino34_hist[i].std(dim='time'))).compute()\n",
    "\n",
    "# for i,item in enumerate(nino34_ssp):\n",
    "#     nino34_ssp[i] =  ((nino34_ssp[i] - (nino34_ssp[i].mean(dim='time'))).compute()/(nino34_ssp[i].std(dim='time'))).compute()\n",
    "\n",
    "\n",
    "## Resample from Monthly to seasonal timesteps\n",
    "nino34_hist = [a.resample(time=\"QS\", label='left').mean() for a in nino34_hist]\n",
    "nino34_ssp = [a.resample(time=\"QS\", label='left').mean() for a in nino34_ssp]\n",
    "\n",
    "# Convert to dataarray\n",
    "nino34_hist = [a.to_array() for a in nino34_hist]\n",
    "nino34_ssp = [a.to_array() for a in nino34_ssp]\n",
    "\n",
    "# Delete useless empty dimension\n",
    "nino34_hist = [nino.squeeze().rename(variable = \"tos\").drop(\"tos\") for nino in nino34_hist]\n",
    "nino34_ssp = [nino.squeeze().rename(variable = \"tos\").drop(\"tos\") for nino in nino34_ssp]\n",
    "\n",
    "# Correct for spurious dimension\n",
    "for i, item in enumerate(nino34_hist):\n",
    "    if len(nino34_hist[i].shape)!=1:\n",
    "        nino34_hist[i] = nino34_hist[i][1]\n",
    "    else:\n",
    "        None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open LAND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'nep'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)                                              # to retrieve netcdf original ATTRIBUTES\n",
    "\n",
    "ds_hist_nep = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    ds_hist_nep.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_nep = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_nep.append(content)    \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_nep = [a.sel(lat=slice(-30,30)) for a in ds_hist_nep]\n",
    "ds_ssp_nep = [a.sel(lat=slice(-30,30)) for a in ds_ssp_nep]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_nep):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_nep[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_nep):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_nep[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_nep = [a.sel(time = slice(\"1901-01\", \"1960-12\")) for a in ds_hist_nep]\n",
    "ds_ssp_nep = [a.sel(time = slice(\"2041-01\", \"2100-12\")) for a in ds_ssp_nep]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_nep = [detrend_dim(a.sel(time = slice(\"1901-01\", \"1960-12\")), \"time\", 1) for a in ds_hist_nep]\n",
    "ds_ssp_nep = [detrend_dim(a, \"time\", 1) for a in ds_ssp_nep]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_nep = [a.resample(time=\"QS\", label='left').mean() for a in ds_hist_nep]\n",
    "ds_ssp_nep = [a.resample(time=\"QS\", label='left').mean() for a in ds_ssp_nep]\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_nep[i] = ds_hist_nep[i].where(ds_hist_nep[i] != -3.7270379e-07)\n",
    "        ds_ssp_nep[i] = ds_ssp_nep[i].where(ds_ssp_nep[i] != -6.18386321e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for inverse NEP value in CESM2\n",
    "\n",
    "esm = []\n",
    "for filepath in sorted(glob.glob(os.path.join('F:/Data/analysis/'+'/'+'historical'+'/'+'nep' + '_*_' + 'historical' + '_*'))):     \n",
    "        content = nc.Dataset(filepath).source_id\n",
    "        esm.append(content)   \n",
    "\n",
    "for i,model in enumerate(esm):\n",
    "        if model == \"CESM2-WACCM\":\n",
    "                ds_hist_nep[i] = ds_hist_nep[i]*-1\n",
    "                ds_ssp_nep[i] = ds_ssp_nep[i]*-1\n",
    "        else:\n",
    "                None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'pr'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_pr = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    ds_hist_pr.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_pr = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_pr.append(content)    \n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_pr):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_pr[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_pr):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_pr[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_pr = [a.sel(lat=slice(-30,30)) for a in ds_hist_pr]\n",
    "ds_ssp_pr = [a.sel(lat=slice(-30,30)) for a in ds_ssp_pr]\n",
    "\n",
    "# Correct pr ssp585 UKESM-r4 from lon:192 to lon:191\n",
    "ds_ssp_pr[41] = ds_ssp_pr[41].isel(lon = slice(0,191))\n",
    "# Correct pr hist UKESM-r4 from lon:192 to lon:191\n",
    "ds_hist_pr[42] = ds_hist_pr[42].isel(lon = slice(0,191))\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_pr = [a.sel(time = slice(\"1901-01\", \"1960-12\")) for a in ds_hist_pr]\n",
    "ds_ssp_pr = [a.sel(time = slice(\"2041-01\", \"2100-12\")) for a in ds_ssp_pr]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_pr = [detrend_dim(a, \"time\", 1) for a in ds_hist_pr]\n",
    "ds_ssp_pr = [detrend_dim(a, \"time\", 1) for a in ds_ssp_pr]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_pr = [a.resample(time=\"QS\", label='left').sum() for a in ds_hist_pr]\n",
    "ds_ssp_pr = [a.resample(time=\"QS\", label='left').sum() for a in ds_ssp_pr]\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_pr[i] = ds_hist_pr[i].where(ds_hist_pr[i] != -3.7270379e-07)\n",
    "        ds_ssp_pr[i] = ds_ssp_pr[i].where(ds_ssp_pr[i] != -6.18386321e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'tas'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_tas = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    ds_hist_tas.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_tas = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_tas.append(content)    \n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_tas):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_tas[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_tas):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_tas[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_tas = [a.sel(lat=slice(-30,30)) for a in ds_hist_tas]\n",
    "ds_ssp_tas = [a.sel(lat=slice(-30,30)) for a in ds_ssp_tas]\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_tas = [a.sel(time = slice(\"1901-01\", \"1960-12\")) for a in ds_hist_tas]\n",
    "ds_ssp_tas = [a.sel(time = slice(\"2041-01\", \"2100-12\")) for a in ds_ssp_tas]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_tas = [detrend_dim(a, \"time\", 1) for a in ds_hist_tas]\n",
    "ds_ssp_tas = [detrend_dim(a, \"time\", 1) for a in ds_ssp_tas]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_tas = [a.resample(time=\"QS\", label='left').mean() for a in ds_hist_tas]\n",
    "ds_ssp_tas = [a.resample(time=\"QS\", label='left').mean() for a in ds_ssp_tas]\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_tas[i] = ds_hist_tas[i].where(ds_hist_tas[i] != -3.7270379e-07)\n",
    "        ds_ssp_tas[i] = ds_ssp_tas[i].where(ds_ssp_tas[i] != -6.18386321e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'mrso'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_mrso = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    ds_hist_mrso.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_mrso = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_mrso.append(content)    \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_mrso = [a.sel(lat=slice(-30,30)) for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [a.sel(lat=slice(-30,30)) for a in ds_ssp_mrso]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_mrso):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_mrso[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_mrso):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_mrso[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_mrso = [a.sel(time = slice(\"1901-01\", \"1960-12\")) for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [a.sel(time = slice(\"2041-01\", \"2100-12\")) for a in ds_ssp_mrso]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_mrso = [detrend_dim(a, \"time\", 1) for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [detrend_dim(a, \"time\", 1) for a in ds_ssp_mrso]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_mrso = [a.resample(time=\"QS\", label='left').mean() for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [a.resample(time=\"QS\", label='left').mean() for a in ds_ssp_mrso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'rsds'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_rsds = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    ds_hist_rsds.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_rsds = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_rsds.append(content)    \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_rsds = [a.sel(lat=slice(-30,30)) for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [a.sel(lat=slice(-30,30)) for a in ds_ssp_rsds]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_rsds):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_rsds[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_rsds):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_rsds[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_rsds = [a.sel(time = slice(\"1901-01\", \"1960-12\")) for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [a.sel(time = slice(\"2041-01\", \"2100-12\")) for a in ds_ssp_rsds]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_rsds = [detrend_dim(a, \"time\", 1) for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [detrend_dim(a, \"time\", 1) for a in ds_ssp_rsds]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_rsds = [a.resample(time=\"QS\", label='left').mean() for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [a.resample(time=\"QS\", label='left').mean() for a in ds_ssp_rsds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'hfls'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_hfls = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    ds_hist_hfls.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_hfls = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_hfls.append(content)    \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_hfls = [a.sel(lat=slice(-30,30)) for a in ds_hist_hfls]\n",
    "ds_ssp_hfls = [a.sel(lat=slice(-30,30)) for a in ds_ssp_hfls]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_hfls):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_hfls[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_hfls):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_hfls[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_hfls = [a.sel(time = slice(\"1901-01\", \"1960-12\")) for a in ds_hist_hfls]\n",
    "ds_ssp_hfls = [a.sel(time = slice(\"2041-01\", \"2100-12\")) for a in ds_ssp_hfls]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_hfls = [detrend_dim(a, \"time\", 1) for a in ds_hist_hfls]\n",
    "ds_ssp_hfls = [detrend_dim(a, \"time\", 1) for a in ds_ssp_hfls]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_hfls = [a.resample(time=\"QS\", label='left').mean() for a in ds_hist_hfls]\n",
    "ds_ssp_hfls = [a.resample(time=\"QS\", label='left').mean() for a in ds_ssp_hfls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate in reverse to safely remove elements by index\n",
    "for i in reversed(range(len(files_list))):\n",
    "    if files_list[i].source_id in {\"MRI-ESM2-0\", \"NorESM2-LM\"}:\n",
    "        del ds_hist_nep[i]\n",
    "        del ds_ssp_nep[i]\n",
    "        del ds_hist_pr[i]\n",
    "        del ds_ssp_pr[i]\n",
    "        del ds_hist_mrso[i]\n",
    "        del ds_ssp_mrso[i]\n",
    "        del ds_hist_hfls[i]\n",
    "        del ds_ssp_hfls[i]\n",
    "        del ds_hist_rsds[i]\n",
    "        del ds_ssp_rsds[i]\n",
    "        del ds_hist_tas[i]\n",
    "        del ds_ssp_tas[i]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for lon lat mishape and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the number of longitude points\n",
    "ds_hist_nep = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_nep, ds_hist_hfls)]\n",
    "ds_hist_mrso = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_mrso, ds_hist_hfls)]\n",
    "ds_hist_tas = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_tas, ds_hist_hfls)]\n",
    "ds_hist_rsds = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_rsds, ds_hist_hfls)]\n",
    "ds_hist_pr = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_pr, ds_hist_hfls)]\n",
    "ds_ssp_hfls = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_hfls, ds_ssp_pr)]\n",
    "ds_ssp_rsds = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_rsds, ds_ssp_pr)]\n",
    "ds_ssp_nep = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_nep, ds_ssp_pr)]\n",
    "ds_ssp_mrso = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_mrso, ds_ssp_pr)]\n",
    "ds_ssp_tas = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_tas, ds_ssp_pr)]\n",
    "\n",
    "# Assign the exactly same set of coordinates to ALL the dataarray (avoid duplication due to numerical approximation of lon lat)\n",
    "ds_hist_pr = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_pr,ds_hist_nep)]\n",
    "ds_hist_tas = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_tas,ds_hist_nep)]\n",
    "ds_hist_mrso = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_mrso,ds_hist_nep)]\n",
    "ds_hist_rsds = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_rsds,ds_hist_nep)]\n",
    "ds_hist_hfls = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_hfls,ds_hist_nep)]\n",
    "ds_ssp_hfls = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_hfls,ds_ssp_nep)]\n",
    "ds_ssp_rsds = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_rsds,ds_ssp_nep)]\n",
    "ds_ssp_pr = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_pr,ds_ssp_nep)]\n",
    "ds_ssp_tas = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_tas,ds_ssp_nep)]\n",
    "ds_ssp_mrso = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_mrso,ds_ssp_nep)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teleconnection/Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor_3D(nino34_hist[0],ds_hist_pr[0]).sel(lon= slice(260,340)).plot(cmap = \"RdBu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Reg with ENSO signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSO is linearly removed from original variables signal, The residuals are then used in the ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the residuals of regression with nino34 to isolate its effect on pr and tas\n",
    "seas = \"DJF\"\n",
    "\n",
    "# Reshape variables\n",
    "X1 = [a.sel(time = a.time.dt.season==seas) for a in nino34_hist]\n",
    "Y = [a.sel(time = a.time.dt.season==seas) for a in ds_hist_pr]\n",
    "Y1 = [a.sel(time = a.time.dt.season==seas) for a in ds_hist_tas]\n",
    "Y2 = [a.sel(time = a.time.dt.season==seas) for a in ds_hist_mrso]\n",
    "Y3 = [a.sel(time = a.time.dt.season==seas) for a in ds_hist_rsds]\n",
    "Y4 = [a.sel(time = a.time.dt.season==seas) for a in ds_hist_hfls]\n",
    "\n",
    "res_pr_hist = []\n",
    "res_tas_hist = []\n",
    "res_mrso_hist = []\n",
    "res_rsds_hist = []\n",
    "res_hfls_hist = []\n",
    "for i, item in enumerate(ds_hist_nep):\n",
    "\n",
    "    content = lag_linregress_3D(X1[i],Y[i])\n",
    "    content1 = lag_linregress_3D(X1[i],Y1[i])\n",
    "    content2 = lag_linregress_3D(X1[i],Y2[i])\n",
    "    content3 = lag_linregress_3D(X1[i],Y3[i])\n",
    "    content4 = lag_linregress_3D(X1[i],Y4[i])\n",
    "\n",
    "    res_pr_hist.append(content)\n",
    "    res_tas_hist.append(content1)\n",
    "    res_mrso_hist.append(content2)\n",
    "    res_rsds_hist.append(content3)\n",
    "    res_hfls_hist.append(content4)\n",
    "\n",
    "# Reshape variables\n",
    "X1 = [a.sel(time = a.time.dt.season==seas) for a in nino34_ssp]\n",
    "Y = [a.sel(time = a.time.dt.season==seas) for a in ds_ssp_pr]\n",
    "Y1 = [a.sel(time = a.time.dt.season==seas) for a in ds_ssp_tas]\n",
    "Y2 = [a.sel(time = a.time.dt.season==seas) for a in ds_ssp_mrso]\n",
    "Y3 = [a.sel(time = a.time.dt.season==seas) for a in ds_ssp_rsds]\n",
    "Y4 = [a.sel(time = a.time.dt.season==seas) for a in ds_ssp_hfls]\n",
    "\n",
    "res_pr_ssp = []\n",
    "res_tas_ssp = []\n",
    "res_mrso_ssp = []\n",
    "res_rsds_ssp = []\n",
    "res_hfls_ssp = []\n",
    "for i, item in enumerate(ds_ssp_nep):\n",
    "\n",
    "    content = lag_linregress_3D(X1[i],Y[i])\n",
    "    content1 = lag_linregress_3D(X1[i],Y1[i])\n",
    "    content2 = lag_linregress_3D(X1[i],Y2[i])\n",
    "    content3 = lag_linregress_3D(X1[i],Y3[i])\n",
    "    content4 = lag_linregress_3D(X1[i],Y4[i])\n",
    "\n",
    "    res_pr_ssp.append(content)\n",
    "    res_tas_ssp.append(content1)\n",
    "    res_mrso_ssp.append(content2)\n",
    "    res_rsds_ssp.append(content3)\n",
    "    res_hfls_ssp.append(content4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# mask = res_mrso_hist[i].rename(\"mrso\").sel(lon= slice(260,340)).stack(cell = [\"lon\",\"lat\"]).dropna(dim = \"cell\", how = \"any\").cell.values\n",
    "# test = res_pr_hist[i].sel(lon= slice(260,340)).stack(cell = [\"lon\",\"lat\"])\n",
    "# test.where(test.cell.values == mask)#.dropna(dim = \"cell\", how = \"any\")\n",
    "# # test.cell\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation\n",
    "\n",
    "To avoid Singular Matrix error while performing regression it's necessary to fill NaN NOT with zero but with a not-zero number (eg 0.001) \n",
    "\n",
    "By standardizing Soil Moisturre SSP with historical Soil Moisture we get infinite values for some realization. Thus substitute inf with 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "historical_ACCESS-ESM1-5\n",
      "historical_ACCESS-ESM1-5\n",
      "historical_ACCESS-ESM1-5\n",
      "historical_ACCESS-ESM1-5\n",
      "historical_ACCESS-ESM1-5\n",
      "historical_BCC-CSM2-MR\n",
      "historical_CESM2-WACCM\n",
      "historical_CESM2-WACCM\n",
      "historical_CESM2-WACCM\n",
      "historical_CMCC-ESM2\n",
      "historical_CNRM-ESM2-1\n",
      "historical_CNRM-ESM2-1\n",
      "historical_CNRM-ESM2-1\n",
      "historical_CNRM-ESM2-1\n",
      "historical_CNRM-ESM2-1\n",
      "historical_CanESM5\n",
      "historical_CanESM5\n",
      "historical_CanESM5\n",
      "historical_CanESM5\n",
      "historical_CanESM5\n",
      "historical_E3SM-1-1-ECA\n",
      "historical_IPSL-CM6A-LR\n",
      "historical_IPSL-CM6A-LR\n",
      "historical_IPSL-CM6A-LR\n",
      "historical_IPSL-CM6A-LR\n",
      "historical_IPSL-CM6A-LR\n",
      "historical_MIROC-ES2L\n",
      "historical_MIROC-ES2L\n",
      "historical_MIROC-ES2L\n",
      "historical_MIROC-ES2L\n",
      "historical_MIROC-ES2L\n",
      "historical_MPI-ESM1-2-LR\n",
      "historical_MPI-ESM1-2-LR\n",
      "historical_MPI-ESM1-2-LR\n",
      "historical_MPI-ESM1-2-LR\n",
      "historical_MPI-ESM1-2-LR\n",
      "historical_MRI-ESM2-0\n",
      "historical_NorESM2-LM\n",
      "historical_NorESM2-MM\n",
      "historical_TaiESM1\n",
      "historical_UKESM1-0-LL\n",
      "historical_UKESM1-0-LL\n",
      "historical_UKESM1-0-LL\n",
      "ssp585_ACCESS-ESM1-5\n",
      "ssp585_ACCESS-ESM1-5\n",
      "ssp585_ACCESS-ESM1-5\n",
      "ssp585_ACCESS-ESM1-5\n",
      "ssp585_ACCESS-ESM1-5\n",
      "ssp585_BCC-CSM2-MR\n",
      "ssp585_CESM2-WACCM\n",
      "ssp585_CESM2-WACCM\n",
      "ssp585_CESM2-WACCM\n",
      "ssp585_CMCC-ESM2\n",
      "ssp585_CNRM-ESM2-1\n",
      "ssp585_CNRM-ESM2-1\n",
      "ssp585_CNRM-ESM2-1\n",
      "ssp585_CNRM-ESM2-1\n",
      "ssp585_CNRM-ESM2-1\n",
      "ssp585_CanESM5\n",
      "ssp585_CanESM5\n",
      "ssp585_CanESM5\n",
      "ssp585_CanESM5\n",
      "ssp585_CanESM5\n",
      "ssp585_E3SM-1-1-ECA\n",
      "ssp585_IPSL-CM6A-LR\n",
      "ssp585_IPSL-CM6A-LR\n",
      "ssp585_IPSL-CM6A-LR\n",
      "ssp585_IPSL-CM6A-LR\n",
      "ssp585_IPSL-CM6A-LR\n",
      "ssp585_MIROC-ES2L\n",
      "ssp585_MIROC-ES2L\n",
      "ssp585_MIROC-ES2L\n",
      "ssp585_MIROC-ES2L\n",
      "ssp585_MIROC-ES2L\n",
      "ssp585_MPI-ESM1-2-LR\n",
      "ssp585_MPI-ESM1-2-LR\n",
      "ssp585_MPI-ESM1-2-LR\n",
      "ssp585_MPI-ESM1-2-LR\n",
      "ssp585_MPI-ESM1-2-LR\n",
      "ssp585_MRI-ESM2-0\n",
      "ssp585_NorESM2-LM\n",
      "ssp585_NorESM2-MM\n",
      "ssp585_TaiESM1\n",
      "ssp585_UKESM1-0-LL\n",
      "ssp585_UKESM1-0-LL\n",
      "ssp585_UKESM1-0-LL\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from regressors import stats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define regression technique\n",
    "# Ridge Regression with Cross-Validation\n",
    "# alphas = np.linspace(.000001, 1)\n",
    "# alphas = np.logspace(-6, 2, 10)\n",
    "# ridge = linear_model.RidgeCV(alphas = alphas, cv =5)\n",
    "\n",
    "# Ridge Regression without Cross-Validation\n",
    "ridge = linear_model.Ridge()\n",
    "\n",
    "# historical\n",
    "coef_ridge_hist = []\n",
    "r2_ridge_hist = []\n",
    "pred_ridge_hist = []\n",
    "pval_ridge_hist = []\n",
    "for i, item in enumerate(ds_hist_nep):\n",
    "    \n",
    "    # if esm[i] == 'CESM2-WACCM':\n",
    "        \n",
    "        #1. Reshape variables\n",
    "        ensohist = nino34_hist[i].rename(\"sst\").sel(time = nino34_hist[i].time.dt.season==seas).sel(time = slice(\"1901-01\", \"1960-12\"))\n",
    "        X1hist = res_pr_hist[i].rename(\"pr\").sel(lon= slice(260,340))\n",
    "        X2hist = res_tas_hist[i].rename(\"tas\").sel(lon= slice(260,340))\n",
    "        X3hist = res_mrso_hist[i].rename(\"mrso\").fillna(0.001).sel(lon= slice(260,340))\n",
    "        X4hist = res_rsds_hist[i].rename(\"rsds\").sel(lon= slice(260,340))\n",
    "        X5hist = res_hfls_hist[i].rename(\"hfls\").sel(lon= slice(260,340))\n",
    "\n",
    "        Y = ds_hist_nep[i].sel(lon= slice(260,340)).fillna(0.001).sel(time = ds_hist_nep[i].time.dt.season==seas)\n",
    "\n",
    "        #2. Standardize predictors\n",
    "        ensohist = ensohist/ensohist.std(dim = [\"time\"])\n",
    "        X1hist = X1hist/X1hist.std(dim = \"time\")\n",
    "        X2hist = X2hist/X2hist.std(dim = \"time\")\n",
    "        X3hist = (X3hist/X3hist.std(dim = \"time\")).fillna(0.001)\n",
    "        X4hist = (X4hist/X4hist.std(dim = \"time\"))\n",
    "        X5hist = X5hist/X5hist.std(dim = \"time\")\n",
    "\n",
    "        #3. Stack on 1D vector\n",
    "        ensohist = ensohist.stack(cell = [\"time\"])\n",
    "        X1hist = X1hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X2hist = X2hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X3hist = X3hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X4hist = X4hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X5hist = X5hist.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "        Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "        #4. Create Dataarray with ENSO signal in every grid cell to perform regression\n",
    "        Xenso = xr.DataArray(data=None, coords=[X1hist.time, X1hist.cell], dims=[\"time\",\"cell\"])\n",
    "        for c in Xenso.cell:\n",
    "            locator = {'cell':c}\n",
    "            Xenso.loc[locator] = ensohist.values\n",
    "            Xenso.values = Xenso.values.astype(\"float64\")       # To avoid the resulting values are \"object\" rather than \"float64\" \n",
    "        Xenso = Xenso.rename(\"sst\")\n",
    "\n",
    "        #5. Create empty dataarray to store regression coefficients\n",
    "        coef = xr.DataArray(data=None, coords=[X1hist.time,X1hist.cell], dims=[\"time\",\"cell\"])\n",
    "        coef[\"pr\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"tas\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        # coef[\"mrso\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"sst\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"rsds\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"hfls\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty dataarray to store p-values\n",
    "        pval = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"pr\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"tas\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        # pval[\"mrso\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"sst\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"rsds\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"hfls\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty datarray to store R2 and Y predicted \n",
    "        r_squared = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pred = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\",\"cell\"])\n",
    "        pred[\"nep\"] = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\", \"cell\"])\n",
    "\n",
    "        #6. Iterate over cells (lan*lon)\n",
    "        for c in X1hist.cell:\n",
    "            locator = {'cell':c}\n",
    "\n",
    "            # merge predictors in one numpy array\n",
    "            df = np.array((X1hist.loc[locator].values, X2hist.loc[locator].values, X4hist.loc[locator].values, X5hist.loc[locator].values, Xenso.loc[locator].values)); df = df.T\n",
    "            \n",
    "            model = ridge.fit(df,Y.loc[locator].values)\n",
    "            r2 = ridge.score(df,Y.loc[locator])\n",
    "            ypred = ridge.predict(df)\n",
    "\n",
    "            # coef[\"pr\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"tas\"].loc[locator] = model.coef_[1]\n",
    "            coef[\"pr\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"rsds\"].loc[locator] = model.coef_[2]\n",
    "            coef[\"hfls\"].loc[locator] = model.coef_[3]\n",
    "            coef[\"sst\"].loc[locator] = model.coef_[4]\n",
    "\n",
    "            # # p-value of the index \"0\" refers to the intercept\n",
    "            # pval[\"pr\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"tas\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[2]\n",
    "            pval[\"pr\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"rsds\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[3]\n",
    "            pval[\"hfls\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[4]\n",
    "            pval[\"sst\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[5]\n",
    "            \n",
    "            r_squared.loc[locator] = r2\n",
    "            pred[\"nep\"].loc[locator] = ypred\n",
    "\n",
    "        #7. Unstack array\n",
    "        coef = coef.unstack()\n",
    "        coef = xr.concat([coef.pr, coef.tas, coef.rsds, coef.hfls, coef.sst], dim = \"coefficients\")     #ORDER of the coefficients\n",
    "        coef[\"pr\"] = coef.pr.astype(np.float64)\n",
    "        coef[\"tas\"] = coef.tas.astype(np.float64)\n",
    "        # coef[\"mrso\"] = coef.mrso.astype(np.float64)\n",
    "        coef[\"rsds\"] = coef.rsds.astype(np.float64)\n",
    "        coef[\"hfls\"] = coef.hfls.astype(np.float64)\n",
    "        coef[\"sst\"] = coef.sst.astype(np.float64)\n",
    "\n",
    "        pval = pval.unstack()\n",
    "        pval = xr.concat([pval.pr, pval.tas, pval.rsds, pval.hfls, pval.sst], dim = \"p-values\")\n",
    "        pval[\"pr\"] = pval.pr.astype(np.float64)\n",
    "        pval[\"tas\"] = pval.tas.astype(np.float64)\n",
    "        # pval[\"mrso\"] = pval.mrso.astype(np.float64)\n",
    "        pval[\"rsds\"] = pval.rsds.astype(np.float64)\n",
    "        pval[\"hfls\"] = pval.hfls.astype(np.float64)\n",
    "        pval[\"sst\"] = pval.sst.astype(np.float64)\n",
    "        \n",
    "        r_squared = r_squared.astype(np.float64).unstack()\n",
    "        pred = pred.astype(np.float64).unstack()\n",
    "\n",
    "        coef_ridge_hist.append(coef)\n",
    "        r2_ridge_hist.append(r_squared)\n",
    "        pred_ridge_hist.append(pred)\n",
    "        pval_ridge_hist.append(pval)\n",
    "        print(\"historical_\" + esm[i])\n",
    "\n",
    "    # else:\n",
    "    #     None\n",
    "\n",
    "# SSP585\n",
    "coef_ridge_ssp = []\n",
    "r2_ridge_ssp = []\n",
    "pred_ridge_ssp = []\n",
    "pval_ridge_ssp = []\n",
    "for i, item in enumerate(ds_ssp_nep):\n",
    "\n",
    "    # if esm[i] == 'CESM2-WACCM':\n",
    "\n",
    "        # Reshape variables\n",
    "        ensossp = nino34_ssp[i].rename(\"sst\").sel(time = nino34_ssp[i].time.dt.season==seas).sel(time = slice(\"2041-01\", \"2100-12\"))\n",
    "        X1ssp = res_pr_ssp[i].rename(\"pr\").sel(lon= slice(260,340))\n",
    "        X2ssp = res_tas_ssp[i].rename(\"tas\").sel(lon= slice(260,340))\n",
    "        X3ssp = res_mrso_ssp[i].rename(\"mrso\").fillna(0.001).sel(lon= slice(260,340))\n",
    "        X4ssp = res_rsds_ssp[i].rename(\"rsds\").sel(lon= slice(260,340))\n",
    "        X5ssp = res_hfls_ssp[i].rename(\"hfls\").sel(lon= slice(260,340))\n",
    "\n",
    "        Y = ds_ssp_nep[i].sel(lon= slice(260,340)).fillna(0.001).sel(time = ds_ssp_nep[i].time.dt.season==seas)\n",
    "\n",
    "        # Need to redefine historical dataset for standardization \n",
    "        ensohist = nino34_hist[i].rename(\"sst\").sel(time = nino34_hist[i].time.dt.season==seas).sel(time = slice(\"1901-01\", \"1960-12\"))\n",
    "        X1hist = res_pr_hist[i].rename(\"pr\").sel(lon= slice(260,340))\n",
    "        X2hist = res_tas_hist[i].rename(\"tas\").sel(lon= slice(260,340))\n",
    "        X3hist = res_mrso_hist[i].rename(\"mrso\").fillna(0.001).sel(lon= slice(260,340))\n",
    "        X4hist = res_rsds_hist[i].rename(\"rsds\").sel(lon= slice(260,340))\n",
    "        X5hist = res_hfls_hist[i].rename(\"hfls\").sel(lon= slice(260,340))\n",
    "\n",
    "        # Standardize predictors\n",
    "        ensossp = ensossp/ensohist.std(dim = [\"time\"])\n",
    "        X1ssp = X1ssp/X1hist.std(dim = \"time\")\n",
    "        X2ssp = X2ssp/X2hist.std(dim = \"time\")\n",
    "        X3ssp = xr.where((X3ssp/X3hist.std(dim = \"time\")).fillna(0.001) == np.inf, 0.001, (X3ssp/X3hist.std(dim = \"time\"))).fillna(0.001)     # sometimes it could happen to have Inf values\n",
    "        X4ssp = (X4ssp/X4hist.std(dim = \"time\"))\n",
    "        X5ssp = (X5ssp/X5hist.std(dim = \"time\"))\n",
    "\n",
    "        # Stack on 1D vector\n",
    "        ensossp = ensossp.stack(cell = [\"time\"])\n",
    "        X1ssp = X1ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X2ssp = X2ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X3ssp = X3ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X4ssp = X4ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X5ssp = X5ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "        Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "        \n",
    "        # Create Dataarray with ENSO signal in every grid cell to perform regression\n",
    "        Xenso = xr.DataArray(data=None, coords=[X1ssp.time, X1ssp.cell], dims=[\"time\",\"cell\"])\n",
    "        for c in Xenso.cell:\n",
    "            locator = {'cell':c}\n",
    "            Xenso.loc[locator] = ensossp.values\n",
    "            Xenso.values = Xenso.values.astype(\"float64\")       # To avoid the resulting values are \"object\" rather than \"float64\" \n",
    "        Xenso = Xenso.rename(\"sst\")\n",
    "\n",
    "        # Create empty dataarray to store regression coefficients\n",
    "        coef = xr.DataArray(data=None, coords=[X1ssp.time,X1ssp.cell], dims=[\"time\",\"cell\"])\n",
    "        coef[\"pr\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"tas\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        # coef[\"mrso\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"sst\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"rsds\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"hfls\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty dataarray to store p-values\n",
    "        pval = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"pr\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"tas\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        # pval[\"mrso\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"sst\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"rsds\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"hfls\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty datarray to store R2 and Y predicted \n",
    "        r_squared = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pred = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\",\"cell\"])\n",
    "        pred[\"nep\"] = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\", \"cell\"])\n",
    "\n",
    "        for c in X1ssp.cell:\n",
    "            locator = {'cell':c}\n",
    "\n",
    "            # merge predictors in one dataarray\n",
    "            df = np.array((X1ssp.loc[locator].values, X2ssp.loc[locator].values, X4ssp.loc[locator].values, X5ssp.loc[locator].values,Xenso.loc[locator].values)); df = df.T\n",
    "            model = ridge.fit(df,Y.loc[locator].values)\n",
    "            r2 = ridge.score(df,Y.loc[locator])\n",
    "            ypred = ridge.predict(df)\n",
    "\n",
    "            coef[\"pr\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"tas\"].loc[locator] = model.coef_[1]\n",
    "            # coef[\"mrso\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"rsds\"].loc[locator] = model.coef_[2]\n",
    "            coef[\"hfls\"].loc[locator] = model.coef_[3]\n",
    "            coef[\"sst\"].loc[locator] = model.coef_[4]\n",
    "\n",
    "            # p-value of the index \"0\" refers to the intercept\n",
    "            pval[\"pr\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"tas\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[2]\n",
    "            # pval[\"mrso\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"rsds\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[3]\n",
    "            pval[\"hfls\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[4]\n",
    "            pval[\"sst\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[5]\n",
    "            \n",
    "            r_squared.loc[locator] = r2\n",
    "            pred[\"nep\"].loc[locator] = ypred\n",
    "\n",
    "        coef = coef.unstack()\n",
    "        coef = xr.concat([coef.pr, coef.tas, coef.rsds, coef.hfls, coef.sst], dim = \"coefficients\")\n",
    "        coef[\"pr\"] = coef.pr.astype(np.float64)\n",
    "        coef[\"tas\"] = coef.tas.astype(np.float64)\n",
    "        # coef[\"mrso\"] = coef.mrso.astype(np.float64)\n",
    "        coef[\"rsds\"] = coef.rsds.astype(np.float64)\n",
    "        coef[\"hfls\"] = coef.hfls.astype(np.float64)\n",
    "        coef[\"sst\"] = coef.sst.astype(np.float64)\n",
    "\n",
    "        pval = pval.unstack()\n",
    "        pval = xr.concat([pval.pr, pval.tas, pval.rsds, pval.hfls, pval.sst], dim = \"p-values\")\n",
    "        pval[\"pr\"] = pval.pr.astype(np.float64)\n",
    "        pval[\"tas\"] = pval.tas.astype(np.float64)\n",
    "        # pval[\"mrso\"] = pval.mrso.astype(np.float64)\n",
    "        pval[\"rsds\"] = pval.rsds.astype(np.float64)\n",
    "        pval[\"hfls\"] = pval.hfls.astype(np.float64)\n",
    "        pval[\"sst\"] = pval.sst.astype(np.float64)\n",
    "        \n",
    "        r_squared = r_squared.astype(np.float64).unstack()\n",
    "        pred = pred.astype(np.float64).unstack()\n",
    "\n",
    "        coef_ridge_ssp.append(coef)\n",
    "        r2_ridge_ssp.append(r_squared)\n",
    "        pred_ridge_ssp.append(pred)\n",
    "        pval_ridge_ssp.append(pval)\n",
    "        print(\"ssp585_\" + esm[i])\n",
    "        \n",
    "    # else:\n",
    "    #     None\n",
    "    \n",
    "\n",
    "# Save and export regression list data\n",
    "import pickle\n",
    "data_path = 'G:/Shared drives/Amazon_ENSO_work/analysis/'\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_coef_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(coef_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_coef_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(coef_ridge_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_pval_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_pval_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_ridge_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_r2_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(r2_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_r2_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(r2_ridge_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_ypred_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pred_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_ypred_g\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pred_ridge_ssp, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open regression list data\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_path = 'G:/Shared drives/Amazon_ENSO_work/analysis/'\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_coef_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    coef_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_coef_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    coef_ridge_ssp = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_pval_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pval_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_pval_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pval_ridge_ssp = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_r2_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    r2_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_r2_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    r2_ridge_ssp = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_ypred_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pred_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_ypred_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pred_ridge_ssp = pickle.load(fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# stack variables over lon-lat dimensions\n",
    "# Xenso = nino34_hist[0].rename(\"sst\")#; Xenso = Xenso.stack(cell = [\"time\"])\n",
    "X1 = ds_hist_pr[0].rename(\"pr\").sel(lon= slice(260,340))\n",
    "X2 = ds_hist_tas[0].rename(\"tas\").sel(lon= slice(260,340))\n",
    "X3 = ds_hist_mrso[0].rename(\"mrso\").sel(lon= slice(260,340), lat = slice(-30,30)).fillna(0)\n",
    "Y = ds_hist_nep[0].sel(lon= slice(260,340), lat = slice(-30,30)).fillna(0)\n",
    "\n",
    "# Standardize predictors\n",
    "X1 = X1/X1.std(dim = \"time\")\n",
    "X2 = X2/X2.std(dim = \"time\")\n",
    "X3 = (X3/X3.std(dim = \"time\")).fillna(0)\n",
    "\n",
    "# Stack on 1D vector\n",
    "X1 = X1.stack(cell = [\"lon\",\"lat\"])\n",
    "X2 = X2.stack(cell = [\"lon\",\"lat\"])\n",
    "X3 = X3.stack(cell = [\"lon\",\"lat\"])\n",
    "Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "# test with ENSO signal in every grid cell\n",
    "# test = xr.DataArray(data=None, coords=[X1.time, X1.cell], dims=[\"time\",\"cell\"])\n",
    "# for i in test.cell:\n",
    "#     locator = {'cell':i}\n",
    "#     test.loc[locator] = Xenso.values\n",
    "# test = test.rename(\"sst\")\n",
    "\n",
    "# merge predictors in one dataarray\n",
    "X = xr.merge([X1, X2, X3]).to_array().transpose(\"time\", \"variable\", \"cell\")\n",
    "\n",
    "# Define RF parameters\n",
    "rf_regr = RandomForestRegressor(n_estimators=100,\n",
    "                              criterion = \"squared_error\",\n",
    "                              max_depth=10,\n",
    "                              max_samples=0.7,\n",
    "                              random_state=0)\n",
    "\n",
    "# Create empty dataarray to store regression coefficients\n",
    "coef = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "coef[\"pr\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "coef[\"tas\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "coef[\"mrso\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "# coef[\"sst\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "\n",
    "for i in X.cell:\n",
    "    locator = {'cell':i}\n",
    "    model = rf_regr.fit(X.loc[locator],Y.loc[locator])\n",
    "    coef[\"pr\"].loc[locator] = model.feature_importances_[0]\n",
    "    coef[\"tas\"].loc[locator] = model.feature_importances_[1]\n",
    "    coef[\"mrso\"].loc[locator] = model.feature_importances_[2]\n",
    "\n",
    "coef = coef.unstack()\n",
    "pr_ridge = coef.pr.astype(np.float64)\n",
    "tas_ridge = coef.tas.astype(np.float64)\n",
    "mrso_ridge = coef.mrso.astype(np.float64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_clim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
