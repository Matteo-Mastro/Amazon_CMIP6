{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = r'C:/Users/mastr/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Windows OS\n",
    "# os.environ['PROJ_LIB'] = r'/Users/mmastro/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Mac OS\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#from scipy.signal import argrelextrema                      # Find local Maxima-Minima in numpy array\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_dim(da, dim, degree):\n",
    "    # Store original attributes\n",
    "    original_attrs = da.attrs\n",
    "\n",
    "    # Detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=degree)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    da_det = da - fit\n",
    "    \n",
    "    # Restore original attributes\n",
    "    da_det.attrs = original_attrs\n",
    "    \n",
    "    return da_det\n",
    "\n",
    "def xr_mean_list(xarray_list):\n",
    "    # Step 1: Group the xarray objects by their \"esm\" attribute\n",
    "    esm_groups = {}\n",
    "    for xr_obj in xarray_list:\n",
    "        esm_value = xr_obj.attrs.get('esm', None)\n",
    "        if esm_value not in esm_groups:\n",
    "            esm_groups[esm_value] = []\n",
    "        esm_groups[esm_value].append(xr_obj)\n",
    "\n",
    "    # Step 2: Sort the esm groups alphabetically by their esm value\n",
    "    sorted_esm_values = sorted(esm_groups.keys())\n",
    "\n",
    "    # Step 3: Calculate the mean for each group in alphabetical order\n",
    "    mean_results = {}\n",
    "    for esm_value in sorted_esm_values:\n",
    "        xr_objs = esm_groups[esm_value]\n",
    "        \n",
    "        # Concatenate all xarray objects in this group along a new dimension (e.g., 'stacked_xarrays')\n",
    "        combined = xr.concat(xr_objs, dim='stacked_xarrays')\n",
    "        \n",
    "        # Calculate the mean along the 'stacked_xarrays' dimension\n",
    "        mean_results[esm_value] = combined.mean(dim='stacked_xarrays')\n",
    "    \n",
    "    return mean_results\n",
    "\n",
    "## Fuction for subsetting colormap values ## \n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "## Function for creating a path, if needed ##\n",
    "def checkDir(out_path):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "def lag_linregress_3D(x, y, lagx=0, lagy=0):\n",
    "    \"\"\"\n",
    "    Input: Two xr.Datarrays of any dimensions with the first dim being time. \n",
    "    Thus the input data could be a 1D time series, or for example, have three dimensions (time,lat,lon). \n",
    "    Datasets can be provied in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    Output: Covariance, correlation, regression slope and intercept, p-value, and standard error on regression\n",
    "    between the two datasets along their aligned time dimension.  \n",
    "    Lag values can be assigned to either of the data, with lagx shifting x, and lagy shifting y, with the specified lag amount. \n",
    "    \"\"\" \n",
    "    #1. Ensure that the data are properly alinged to each other. \n",
    "    x,y = xr.align(x,y)\n",
    "    \n",
    "    #2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx!=0:\n",
    "        #If x lags y by 1, x must be shifted 1 step backwards. \n",
    "        #But as the 'zero-th' value is nonexistant, xr assigns it as invalid (nan). Hence it needs to be dropped\n",
    "        x   = x.shift(time = -lagx).dropna(dim='time')\n",
    "        #Next important step is to re-align the two datasets so that y adjusts to the changed coordinates of x\n",
    "        x,y = xr.align(x,y)\n",
    "\n",
    "    if lagy!=0:\n",
    "        y   = y.shift(time = -lagy).dropna(dim='time')\n",
    "        x,y = xr.align(x,y)\n",
    " \n",
    "    #3. Compute data length, mean and standard deviation along time axis for further use: \n",
    "    n     = x.shape[0]\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd  = x.std(axis=0)\n",
    "    ystd  = y.std(axis=0)\n",
    "    \n",
    "    #4. Compute covariance along time axis\n",
    "    cov   =  np.sum((x - xmean)*(y - ymean), axis=0)/(n)\n",
    "    \n",
    "    #5. Compute correlation along time axis\n",
    "    cor   = cov/(xstd*ystd)\n",
    "    \n",
    "    #6. Compute regression slope and intercept:\n",
    "    slope     = cov/(xstd**2)\n",
    "    intercept = ymean - xmean*slope\n",
    "    y_pred =  intercept + slope*x\n",
    "    res = y - y_pred\n",
    "\n",
    "    #7. Compute P-value and standard error\n",
    "    #Compute t-statistics\n",
    "    tstats = cor*np.sqrt(n-2)/np.sqrt(1-cor**2)\n",
    "    stderr = slope/tstats\n",
    "    \n",
    "    from scipy.stats import t\n",
    "    pval   = t.sf(tstats, n-2)*2\n",
    "    pval   = xr.DataArray(pval, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    #return cov,cor,slope,intercept,pval,stderr\n",
    "    return res\n",
    "\n",
    "def cor_3D(x, y, lagx=0, lagy=0):\n",
    "    \"\"\"\n",
    "    Input: Two xr.Datarrays of any dimensions with the first dim being time. \n",
    "    Thus the input data could be a 1D time series, or for example, have three dimensions (time,lat,lon). \n",
    "    Datasets can be provied in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    Output: Covariance, correlation, regression slope and intercept, p-value, and standard error on regression\n",
    "    between the two datasets along their aligned time dimension.  \n",
    "    Lag values can be assigned to either of the data, with lagx shifting x, and lagy shifting y, with the specified lag amount. \n",
    "    \"\"\" \n",
    "    #1. Ensure that the data are properly alinged to each other. \n",
    "    x,y = xr.align(x,y)\n",
    "    \n",
    "    #2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx!=0:\n",
    "        #If x lags y by 1, x must be shifted 1 step backwards. \n",
    "        #But as the 'zero-th' value is nonexistant, xr assigns it as invalid (nan). Hence it needs to be dropped\n",
    "        x   = x.shift(time = -lagx).dropna(dim='time')\n",
    "        #Next important step is to re-align the two datasets so that y adjusts to the changed coordinates of x\n",
    "        x,y = xr.align(x,y)\n",
    "\n",
    "    if lagy!=0:\n",
    "        y   = y.shift(time = -lagy).dropna(dim='time')\n",
    "        x,y = xr.align(x,y)\n",
    " \n",
    "    #3. Compute data length, mean and standard deviation along time axis for further use: \n",
    "    n     = x.shape[0]\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd  = x.std(axis=0)\n",
    "    ystd  = y.std(axis=0)\n",
    "    \n",
    "    #4. Compute covariance along time axis\n",
    "    cov   =  np.sum((x - xmean)*(y - ymean), axis=0)/(n)\n",
    "    \n",
    "    #5. Compute correlation along time axis\n",
    "    cor   = cov/(xstd*ystd)\n",
    "\n",
    "    #return cov,cor,slope,intercept,pval,stderr\n",
    "    return cor\n",
    "\n",
    "def xr_multipletest(p, alpha=0.05, method='fdr_bh', **multipletests_kwargs):\n",
    "    \"\"\"Apply statsmodels.stats.multitest.multipletests for multi-dimensional xr.objects.\"\"\"\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    # stack all to 1d array\n",
    "    p_stacked = p.stack(s=p.dims)\n",
    "    # mask only where not nan: https://github.com/statsmodels/statsmodels/issues/2899\n",
    "    mask = np.isfinite(p_stacked)\n",
    "    pvals_corrected = np.full(p_stacked.shape, np.nan)\n",
    "    reject = np.full(p_stacked.shape, np.nan)\n",
    "    # apply test where mask\n",
    "    reject[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[0]\n",
    "    pvals_corrected[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[1]\n",
    "\n",
    "    def unstack(reject, p_stacked):\n",
    "        \"\"\"Exchange values from p_stacked with reject (1d array) and unstack.\"\"\"\n",
    "        xreject = p_stacked.copy()\n",
    "        xreject.values = reject\n",
    "        xreject = xreject.unstack()\n",
    "        return xreject\n",
    "\n",
    "    reject = unstack(reject, p_stacked)\n",
    "    pvals_corrected = unstack(pvals_corrected, p_stacked)\n",
    "    return reject, pvals_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Path for the output files (images, etc)\n",
    "# out_path = 'C:/Users/mastr/Documents/Amazon/RESULTS/'\n",
    "out_path = 'G:/Shared drives/Amazon_ENSO_work/RESULTS/MLR/'\n",
    "\n",
    "#out_path = \"D:/Data/CMIP6/RESULTS\"\n",
    "\n",
    "# -- Create directories\n",
    "checkDir(out_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open SST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------- Open data (MODEL) ------- #####\n",
    "data_path = 'F:/Data/analysis/'\n",
    "scenario = 'historical'\n",
    "files = \"nino34\" + '_*_' + scenario + '_*'\n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "nino34_hist = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"]); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    nino34_hist.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = \"nino34\" + '_*_' + scenario + '_*' \n",
    "\n",
    "nino34_ssp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"]); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    nino34_ssp.append(content)\n",
    "\n",
    "## Normalization\n",
    "# for i,item in enumerate(nino34_hist):\n",
    "#     nino34_hist[i] =  ((nino34_hist[i] - (nino34_hist[i].mean(dim='time'))).compute()/(nino34_hist[i].std(dim='time'))).compute()\n",
    "\n",
    "# for i,item in enumerate(nino34_ssp):\n",
    "#     nino34_ssp[i] =  ((nino34_ssp[i] - (nino34_ssp[i].mean(dim='time'))).compute()/(nino34_ssp[i].std(dim='time'))).compute()\n",
    "\n",
    "\n",
    "## Resample from Monthly to seasonal timesteps\n",
    "nino34_hist = [a.resample(time=\"Y\", label='left').mean() for a in nino34_hist]\n",
    "nino34_ssp = [a.resample(time=\"Y\", label='left').mean() for a in nino34_ssp]\n",
    "\n",
    "# Convert to dataarray\n",
    "nino34_hist = [a.to_array() for a in nino34_hist]\n",
    "nino34_ssp = [a.to_array() for a in nino34_ssp]\n",
    "\n",
    "# Delete useless empty dimension\n",
    "nino34_hist = [nino.squeeze().rename(variable = \"tos\").drop(\"tos\") for nino in nino34_hist]\n",
    "nino34_ssp = [nino.squeeze().rename(variable = \"tos\").drop(\"tos\") for nino in nino34_ssp]\n",
    "\n",
    "# Correct for spurious dimension\n",
    "for i, item in enumerate(nino34_hist):\n",
    "    if len(nino34_hist[i].shape)!=1:\n",
    "        nino34_hist[i] = nino34_hist[i][1]\n",
    "    else:\n",
    "        None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open LAND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'nbp'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)                                              # to retrieve netcdf original ATTRIBUTES\n",
    "\n",
    "ds_hist_nbp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_hist_nbp.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_nbp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_ssp_nbp.append(content)  \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_nbp = [a.sel(lat=slice(-30,30)) for a in ds_hist_nbp]\n",
    "ds_ssp_nbp = [a.sel(lat=slice(-30,30)) for a in ds_ssp_nbp]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_nbp):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_nbp[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_nbp):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_nbp[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_nbp = [a for a in ds_hist_nbp]\n",
    "ds_ssp_nbp = [a for a in ds_ssp_nbp]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_nbp = [detrend_dim(a, \"time\", 1) for a in ds_hist_nbp]\n",
    "ds_ssp_nbp = [detrend_dim(a, \"time\", 1) for a in ds_ssp_nbp]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_nbp = [a.resample(time=\"Y\", label='left').mean() for a in ds_hist_nbp]\n",
    "ds_ssp_nbp = [a.resample(time=\"Y\", label='left').mean() for a in ds_ssp_nbp]\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_nbp[i] = ds_hist_nbp[i].where(ds_hist_nbp[i] != -3.7270379e-07)\n",
    "        ds_ssp_nbp[i] = ds_ssp_nbp[i].where(ds_ssp_nbp[i] != -6.18386321e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'pr'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_pr = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_hist_pr.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_pr = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_ssp_pr.append(content)   \n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_pr):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_pr[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_pr):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_pr[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_pr = [a.sel(lat=slice(-30,30)) for a in ds_hist_pr]\n",
    "ds_ssp_pr = [a.sel(lat=slice(-30,30)) for a in ds_ssp_pr]\n",
    "\n",
    "# Correct pr ssp585 UKESM-r4 from lon:192 to lon:191\n",
    "ds_ssp_pr[39] = ds_ssp_pr[39].isel(lon = slice(0,191))\n",
    "# Correct pr hist UKESM-r4 from lon:192 to lon:191\n",
    "ds_hist_pr[39] = ds_hist_pr[39].isel(lon = slice(0,191))\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_pr = [a for a in ds_hist_pr]\n",
    "ds_ssp_pr = [a for a in ds_ssp_pr]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_pr = [detrend_dim(a, \"time\", 1) for a in ds_hist_pr]\n",
    "ds_ssp_pr = [detrend_dim(a, \"time\", 1) for a in ds_ssp_pr]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_pr = [a.resample(time=\"Y\", label='left').sum() for a in ds_hist_pr]\n",
    "ds_ssp_pr = [a.resample(time=\"Y\", label='left').sum() for a in ds_ssp_pr]\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_pr[i] = ds_hist_pr[i].where(ds_hist_pr[i] != -3.7270379e-07)\n",
    "        ds_ssp_pr[i] = ds_ssp_pr[i].where(ds_ssp_pr[i] != -6.18386321e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'tas'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_tas = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_hist_tas.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_tas = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_ssp_tas.append(content)  \n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_tas):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_tas[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_tas):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_tas[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_tas = [a.sel(lat=slice(-30,30)) for a in ds_hist_tas]\n",
    "ds_ssp_tas = [a.sel(lat=slice(-30,30)) for a in ds_ssp_tas]\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_tas = [a for a in ds_hist_tas]\n",
    "ds_ssp_tas = [a for a in ds_ssp_tas]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_tas = [detrend_dim(a, \"time\", 1) for a in ds_hist_tas]\n",
    "ds_ssp_tas = [detrend_dim(a, \"time\", 1) for a in ds_ssp_tas]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_tas = [a.resample(time=\"Y\", label='left').mean() for a in ds_hist_tas]\n",
    "ds_ssp_tas = [a.resample(time=\"Y\", label='left').mean() for a in ds_ssp_tas]\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_tas[i] = ds_hist_tas[i].where(ds_hist_tas[i] != -3.7270379e-07)\n",
    "        ds_ssp_tas[i] = ds_ssp_tas[i].where(ds_ssp_tas[i] != -6.18386321e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'mrso'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_mrso = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_hist_mrso.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_mrso = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_ssp_mrso.append(content)   \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_mrso = [a.sel(lat=slice(-30,30)) for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [a.sel(lat=slice(-30,30)) for a in ds_ssp_mrso]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_mrso):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_mrso[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_mrso):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_mrso[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_mrso = [a for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [a for a in ds_ssp_mrso]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_mrso = [detrend_dim(a, \"time\", 1) for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [detrend_dim(a, \"time\", 1) for a in ds_ssp_mrso]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_mrso = [a.resample(time=\"Y\", label='left').mean() for a in ds_hist_mrso]\n",
    "ds_ssp_mrso = [a.resample(time=\"Y\", label='left').mean() for a in ds_ssp_mrso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'rsds'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_rsds = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_hist_rsds.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_rsds = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_ssp_rsds.append(content)    \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_rsds = [a.sel(lat=slice(-30,30)) for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [a.sel(lat=slice(-30,30)) for a in ds_ssp_rsds]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_rsds):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_rsds[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_rsds):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_rsds[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_rsds = [a for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [a for a in ds_ssp_rsds]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_rsds = [detrend_dim(a, \"time\", 1) for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [detrend_dim(a, \"time\", 1) for a in ds_ssp_rsds]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_rsds = [a.resample(time=\"Y\", label='left').mean() for a in ds_hist_rsds]\n",
    "ds_ssp_rsds = [a.resample(time=\"Y\", label='left').mean() for a in ds_ssp_rsds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'hurs'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_hist_hurs = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)\n",
    "    ds_hist_hurs.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "ds_ssp_hurs = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    content = content.assign_attrs(esm=nc.Dataset(filepath).source_id)    \n",
    "    ds_ssp_hurs.append(content)    \n",
    "\n",
    "# Subsetting latitude\n",
    "ds_hist_hurs = [a.sel(lat=slice(-30,30)) for a in ds_hist_hurs]\n",
    "ds_ssp_hurs = [a.sel(lat=slice(-30,30)) for a in ds_ssp_hurs]\n",
    "\n",
    "# Uniform calendar\n",
    "for i, item in enumerate(ds_hist_hurs):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist_hurs[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "for i, item in enumerate(ds_ssp_hurs):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_hurs[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# Select time periods\n",
    "ds_hist_hurs = [a for a in ds_hist_hurs]\n",
    "ds_ssp_hurs = [a for a in ds_ssp_hurs]\n",
    "\n",
    "# Detrending of 1st order\n",
    "ds_hist_hurs = [detrend_dim(a, \"time\", 1) for a in ds_hist_hurs]\n",
    "ds_ssp_hurs = [detrend_dim(a, \"time\", 1) for a in ds_ssp_hurs]\n",
    "\n",
    "# Resample from months to seasons\n",
    "ds_hist_hurs = [a.resample(time=\"Y\", label='left').mean() for a in ds_hist_hurs]\n",
    "ds_ssp_hurs = [a.resample(time=\"Y\", label='left').mean() for a in ds_ssp_hurs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_vpd(rh, tas, unit):\n",
    "    \"\"\"\n",
    "    Input: Relative Humidity and Air Temperature \n",
    "    Output: Vapour Pressure Deficit (VPD), in kPa\n",
    "    \n",
    "    Parameters:\n",
    "    rh : Relative Humidity in %\n",
    "    tas : Air temperature in Kelvin or Celsius\n",
    "    unit : 'Kelvin' or 'Celsius'\n",
    "    \"\"\"\n",
    "    original_attrs = tas.attrs\n",
    "    # Convert temperature to Celsius if given in Kelvin\n",
    "    if unit == \"Kelvin\":\n",
    "        tas_celsius = tas - 273.15\n",
    "    elif unit == \"Celsius\":\n",
    "        tas_celsius = tas\n",
    "    else:\n",
    "        raise ValueError(\"Insert correct Unit of Measure for Temperature: 'Kelvin' or 'Celsius'\")\n",
    "    \n",
    "    # Calculate Saturation Vapour Pressure (SVP) in kPa\n",
    "    svp = 0.61078 * np.exp((17.27 * tas_celsius) / (tas_celsius + 237.3))\n",
    "    # Calculate Vapour Pressure Deficit (VPD) in kPa\n",
    "    vpd = svp * (1 - rh / 100)\n",
    "    vpd = vpd.assign_attrs(original_attrs)\n",
    "    vpd = vpd.assign_attrs(unit = \"kPa\")\n",
    "\n",
    "    return vpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hist_vpd = []; ds_ssp_vpd = []\n",
    "for i, item in enumerate(ds_hist_hurs):\n",
    "    content = calculate_vpd(ds_hist_hurs[i], ds_hist_tas[i], \"Celsius\")\n",
    "    content1 = calculate_vpd(ds_ssp_hurs[i], ds_ssp_tas[i], \"Celsius\")\n",
    "\n",
    "    ds_hist_vpd.append(content)\n",
    "    ds_ssp_vpd.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(ds_hist_vpd):\n",
    "    if item.esm == \"NorESM2-LM\":\n",
    "        ds_hist_vpd[i] = ds_hist_vpd[i].isel(plev = 0)\n",
    "        ds_ssp_vpd[i] = ds_ssp_vpd[i].isel(plev = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_hist_hurs\n",
    "del ds_ssp_hurs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for lon lat mishape and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the number of longitude points\n",
    "ds_hist_nbp = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_nbp, ds_hist_vpd)]\n",
    "ds_hist_mrso = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_mrso, ds_hist_vpd)]\n",
    "ds_hist_tas = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_tas, ds_hist_vpd)]\n",
    "ds_hist_rsds = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_rsds, ds_hist_vpd)]\n",
    "ds_hist_pr = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_hist_pr, ds_hist_vpd)]\n",
    "ds_ssp_vpd = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_vpd, ds_ssp_pr)]\n",
    "ds_ssp_rsds = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_rsds, ds_ssp_pr)]\n",
    "ds_ssp_nbp = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_nbp, ds_ssp_pr)]\n",
    "ds_ssp_mrso = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_mrso, ds_ssp_pr)]\n",
    "ds_ssp_tas = [a.isel(lon = slice(0,b.lon.shape[0])) for a,b in zip(ds_ssp_tas, ds_ssp_pr)]\n",
    "\n",
    "# Assign the exactly same set of coordinates to ALL the dataarray (avoid duplication due to numerical approximation of lon lat)\n",
    "ds_hist_pr = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_pr,ds_hist_nbp)]\n",
    "ds_hist_tas = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_tas,ds_hist_nbp)]\n",
    "ds_hist_mrso = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_mrso,ds_hist_nbp)]\n",
    "ds_hist_rsds = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_rsds,ds_hist_nbp)]\n",
    "ds_hist_vpd = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_hist_vpd,ds_hist_nbp)]\n",
    "ds_ssp_vpd = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_vpd,ds_ssp_nbp)]\n",
    "ds_ssp_rsds = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_rsds,ds_ssp_nbp)]\n",
    "ds_ssp_pr = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_pr,ds_ssp_nbp)]\n",
    "ds_ssp_tas = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_tas,ds_ssp_nbp)]\n",
    "ds_ssp_mrso = [a.assign_coords(lat = b.lat, lon = b.lon) for a,b in zip(ds_ssp_mrso,ds_ssp_nbp)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Reg with ENSO signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSO is linearly removed from original variables signal, The residuals are then used in the ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the residuals of regression with nino34 to isolate its effect on pr and tas\n",
    "seas = \"DJF\"\n",
    "\n",
    "# Reshape variables\n",
    "X1 = [a for a in nino34_hist]\n",
    "Y = [a for a in ds_hist_pr]\n",
    "Y1 = [a for a in ds_hist_tas]\n",
    "Y2 = [a for a in ds_hist_mrso]\n",
    "Y3 = [a for a in ds_hist_rsds]\n",
    "Y4 = [a for a in ds_hist_vpd]\n",
    "\n",
    "res_pr_hist = []\n",
    "res_tas_hist = []\n",
    "res_mrso_hist = []\n",
    "res_rsds_hist = []\n",
    "res_vpd_hist = []\n",
    "for i, item in enumerate(ds_hist_nbp):\n",
    "\n",
    "    content = lag_linregress_3D(X1[i],Y[i])\n",
    "    content1 = lag_linregress_3D(X1[i],Y1[i])\n",
    "    content2 = lag_linregress_3D(X1[i],Y2[i])\n",
    "    content3 = lag_linregress_3D(X1[i],Y3[i])\n",
    "    content4 = lag_linregress_3D(X1[i],Y4[i])\n",
    "\n",
    "    res_pr_hist.append(content)\n",
    "    res_tas_hist.append(content1)\n",
    "    res_mrso_hist.append(content2)\n",
    "    res_rsds_hist.append(content3)\n",
    "    res_vpd_hist.append(content4)\n",
    "\n",
    "# Reshape variables\n",
    "X1 = [a for a in nino34_ssp]\n",
    "Y = [a for a in ds_ssp_pr]\n",
    "Y1 = [a for a in ds_ssp_tas]\n",
    "Y2 = [a for a in ds_ssp_mrso]\n",
    "Y3 = [a for a in ds_ssp_rsds]\n",
    "Y4 = [a for a in ds_ssp_vpd]\n",
    "\n",
    "res_pr_ssp = []\n",
    "res_tas_ssp = []\n",
    "res_mrso_ssp = []\n",
    "res_rsds_ssp = []\n",
    "res_vpd_ssp = []\n",
    "for i, item in enumerate(ds_ssp_nbp):\n",
    "\n",
    "    content = lag_linregress_3D(X1[i],Y[i])\n",
    "    content1 = lag_linregress_3D(X1[i],Y1[i])\n",
    "    content2 = lag_linregress_3D(X1[i],Y2[i])\n",
    "    content3 = lag_linregress_3D(X1[i],Y3[i])\n",
    "    content4 = lag_linregress_3D(X1[i],Y4[i])\n",
    "\n",
    "    res_pr_ssp.append(content)\n",
    "    res_tas_ssp.append(content1)\n",
    "    res_mrso_ssp.append(content2)\n",
    "    res_rsds_ssp.append(content3)\n",
    "    res_vpd_ssp.append(content4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation\n",
    "\n",
    "To avoid Singular Matrix error while performing regression it's necessary to fill NaN NOT with zero but with a not-zero number (eg 0.001) \n",
    "\n",
    "By standardizing Soil Moisturre SSP with historical Soil Moisture we get infinite values for some realization. Thus substitute inf with 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing historical_ACCESS-ESM1-5\n",
      "processing historical_ACCESS-ESM1-5\n",
      "processing historical_ACCESS-ESM1-5\n",
      "processing historical_ACCESS-ESM1-5\n",
      "processing historical_ACCESS-ESM1-5\n",
      "processing historical_CESM2\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from regressors import stats   \n",
    "\n",
    "# Define regression technique\n",
    "# Ridge Regression with Cross-Validation\n",
    "alphas = np.linspace(.000001, 1)\n",
    "alphas = np.logspace(-6, 2, 10)\n",
    "ridge = linear_model.RidgeCV(alphas = alphas, cv =5)\n",
    "\n",
    "# Ridge Regression without Cross-Validation\n",
    "# ridge = linear_model.Ridge()\n",
    "\n",
    "# historical\n",
    "coef_ridge_hist = []\n",
    "r2_ridge_hist = []\n",
    "pred_ridge_hist = []\n",
    "pval_ridge_hist = []\n",
    "for i, item in enumerate(ds_hist_nbp):\n",
    "        print(\"processing historical_\" + ds_hist_nbp[i].esm)\n",
    "\n",
    "    # if esm[i] == 'CESM2-WACCM':\n",
    "        \n",
    "        #1. Reshape variables\n",
    "        ensohist = nino34_hist[i].rename(\"sst\")\n",
    "        X1hist = res_pr_hist[i].rename(\"pr\").sel(lon= slice(260,340))\n",
    "        X2hist = res_tas_hist[i].rename(\"tas\").sel(lon= slice(260,340))\n",
    "        X3hist = res_mrso_hist[i].rename(\"mrso\").fillna(0.001).sel(lon= slice(260,340))\n",
    "        X4hist = res_rsds_hist[i].rename(\"rsds\").sel(lon= slice(260,340))\n",
    "        X5hist = res_vpd_hist[i].rename(\"vpd\").sel(lon= slice(260,340))\n",
    "\n",
    "        Y = ds_hist_nbp[i].sel(lon= slice(260,340)).fillna(0.001)\n",
    "\n",
    "        #2. Standardize predictors\n",
    "        ensohist = ensohist/ensohist.std(dim = [\"time\"])\n",
    "        X1hist = X1hist/X1hist.std(dim = \"time\")\n",
    "        X2hist = X2hist/X2hist.std(dim = \"time\")\n",
    "        X3hist = (X3hist/X3hist.std(dim = \"time\")).fillna(0.001)\n",
    "        X4hist = (X4hist/X4hist.std(dim = \"time\"))\n",
    "        X5hist = X5hist/X5hist.std(dim = \"time\")\n",
    "\n",
    "        #3. Stack on 1D vector\n",
    "        ensohist = ensohist.stack(cell = [\"time\"])\n",
    "        X1hist = X1hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X2hist = X2hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X3hist = X3hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X4hist = X4hist.stack(cell = [\"lon\",\"lat\"])\n",
    "        X5hist = X5hist.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "        Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "        #4. Create Dataarray with ENSO signal in every grid cell to perform regression\n",
    "        Xenso = xr.DataArray(data=None, coords=[X1hist.time, X1hist.cell], dims=[\"time\",\"cell\"])\n",
    "        for c in Xenso.cell:\n",
    "            locator = {'cell':c}\n",
    "            Xenso.loc[locator] = ensohist.values\n",
    "            Xenso.values = Xenso.values.astype(\"float64\")       # To avoid the resulting values are \"object\" rather than \"float64\" \n",
    "        Xenso = Xenso.rename(\"sst\")\n",
    "\n",
    "        #5. Create empty dataarray to store regression coefficients\n",
    "        coef = xr.DataArray(data=None, coords=[X1hist.time,X1hist.cell], dims=[\"time\",\"cell\"])\n",
    "        # coef[\"pr\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"tas\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"mrso\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"sst\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"rsds\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        coef[\"vpd\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty dataarray to store p-values\n",
    "        pval = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        # pval[\"pr\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"tas\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"mrso\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"sst\"] = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"rsds\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pval[\"vpd\"] =  xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty datarray to store R2 and Y predicted \n",
    "        r_squared = xr.DataArray(data=None, coords=[X1hist.cell], dims=[\"cell\"])\n",
    "        pred = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\",\"cell\"])\n",
    "        pred[\"nbp\"] = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\", \"cell\"])\n",
    "\n",
    "        #6. Iterate over cells (lan*lon)\n",
    "        for c in X1hist.cell:\n",
    "            locator = {'cell':c}\n",
    "            # Check if the mrso value is NaN or 0 for this cell\n",
    "            if np.isnan(res_mrso_hist[i].stack(cell = [\"lon\",\"lat\"]).loc[locator].values).any() or (res_mrso_hist[i].stack(cell = [\"lon\",\"lat\"]).loc[locator].values == 0).any():\n",
    "\n",
    "                # Assign NaN to all result arrays for this cell\n",
    "                coef[\"tas\"].loc[locator] = np.nan\n",
    "                coef[\"mrso\"].loc[locator] = np.nan\n",
    "                coef[\"rsds\"].loc[locator] = np.nan\n",
    "                coef[\"vpd\"].loc[locator] = np.nan\n",
    "\n",
    "                pval[\"tas\"].loc[locator] = np.nan\n",
    "                pval[\"mrso\"].loc[locator] = np.nan\n",
    "                pval[\"rsds\"].loc[locator] = np.nan\n",
    "                pval[\"vpd\"].loc[locator] = np.nan\n",
    "\n",
    "                r_squared.loc[locator] = np.nan\n",
    "                pred[\"nbp\"].loc[locator] = np.nan\n",
    "                continue  # Skip this cell and move to the next\n",
    "\n",
    "            # merge predictors in one numpy array\n",
    "            df = np.array((X2hist.loc[locator].values, X3hist.loc[locator].values, X4hist.loc[locator].values, X5hist.loc[locator].values, Xenso.loc[locator].values)); df = df.T\n",
    "            \n",
    "            model = ridge.fit(df,Y.loc[locator].values)\n",
    "            r2 = ridge.score(df,Y.loc[locator])\n",
    "            ypred = ridge.predict(df)\n",
    "\n",
    "            # coef[\"pr\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"tas\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"mrso\"].loc[locator] = model.coef_[1]\n",
    "            coef[\"rsds\"].loc[locator] = model.coef_[2]\n",
    "            coef[\"vpd\"].loc[locator] = model.coef_[3]\n",
    "            coef[\"sst\"].loc[locator] = model.coef_[4]\n",
    "\n",
    "            # # p-value of the index \"0\" refers to the intercept\n",
    "            # pval[\"pr\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"tas\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"mrso\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[2]\n",
    "            pval[\"rsds\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[3]\n",
    "            pval[\"vpd\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[4]\n",
    "            pval[\"sst\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[5]\n",
    "            \n",
    "            r_squared.loc[locator] = r2\n",
    "            pred[\"nbp\"].loc[locator] = ypred\n",
    "\n",
    "        #7. Unstack array\n",
    "        coef = coef.unstack()\n",
    "        coef = xr.concat([coef.tas, coef.mrso, coef.rsds, coef.vpd, coef.sst], dim = \"coefficients\")     #ORDER of the coefficients\n",
    "        # coef[\"pr\"] = coef.pr.astype(np.float64)\n",
    "        coef[\"tas\"] = coef.tas.astype(np.float64)\n",
    "        coef[\"mrso\"] = coef.mrso.astype(np.float64)\n",
    "        coef[\"rsds\"] = coef.rsds.astype(np.float64)\n",
    "        coef[\"vpd\"] = coef.vpd.astype(np.float64)\n",
    "        coef[\"sst\"] = coef.sst.astype(np.float64)\n",
    "\n",
    "        pval = pval.unstack()\n",
    "        pval = xr.concat([pval.tas, pval.mrso, pval.rsds, pval.vpd, pval.sst], dim = \"p-values\")\n",
    "        # pval[\"pr\"] = pval.pr.astype(np.float64)\n",
    "        pval[\"tas\"] = pval.tas.astype(np.float64)\n",
    "        pval[\"mrso\"] = pval.mrso.astype(np.float64)\n",
    "        pval[\"rsds\"] = pval.rsds.astype(np.float64)\n",
    "        pval[\"vpd\"] = pval.vpd.astype(np.float64)\n",
    "        pval[\"sst\"] = pval.sst.astype(np.float64)\n",
    "        \n",
    "        r_squared = r_squared.astype(np.float64).unstack()\n",
    "        pred = pred.astype(np.float64).unstack()\n",
    "\n",
    "        coef_ridge_hist.append(coef)\n",
    "        r2_ridge_hist.append(r_squared)\n",
    "        pred_ridge_hist.append(pred)\n",
    "        pval_ridge_hist.append(pval)\n",
    "\n",
    "    # else:\n",
    "    #     None\n",
    "\n",
    "# SSP585\n",
    "coef_ridge_ssp = []\n",
    "r2_ridge_ssp = []\n",
    "pred_ridge_ssp = []\n",
    "pval_ridge_ssp = []\n",
    "for i, item in enumerate(ds_ssp_nbp):\n",
    "        print(\"processing ssp585_\" + ds_ssp_nbp[i].esm)\n",
    "\n",
    "    # if esm[i] == 'CESM2-WACCM':\n",
    "\n",
    "        # Reshape variables\n",
    "        ensossp = nino34_ssp[i].rename(\"sst\")\n",
    "        X1ssp = res_pr_ssp[i].rename(\"pr\").sel(lon= slice(260,340))\n",
    "        X2ssp = res_tas_ssp[i].rename(\"tas\").sel(lon= slice(260,340))\n",
    "        X3ssp = res_mrso_ssp[i].rename(\"mrso\").fillna(0.001).sel(lon= slice(260,340))\n",
    "        X4ssp = res_rsds_ssp[i].rename(\"rsds\").sel(lon= slice(260,340))\n",
    "        X5ssp = res_vpd_ssp[i].rename(\"vpd\").sel(lon= slice(260,340))\n",
    "\n",
    "        Y = ds_ssp_nbp[i].sel(lon= slice(260,340)).fillna(0.001)\n",
    "\n",
    "        # Need to redefine historical dataset for standardization \n",
    "        ensohist = nino34_hist[i].rename(\"sst\")\n",
    "        X1hist = res_pr_hist[i].rename(\"pr\").sel(lon= slice(260,340))\n",
    "        X2hist = res_tas_hist[i].rename(\"tas\").sel(lon= slice(260,340))\n",
    "        X3hist = res_mrso_hist[i].rename(\"mrso\").fillna(0.001).sel(lon= slice(260,340))\n",
    "        X4hist = res_rsds_hist[i].rename(\"rsds\").sel(lon= slice(260,340))\n",
    "        X5hist = res_vpd_hist[i].rename(\"vpd\").sel(lon= slice(260,340))\n",
    "\n",
    "        # Standardize predictors\n",
    "        ensossp = ensossp/ensohist.std(dim = [\"time\"])\n",
    "        X1ssp = X1ssp/X1hist.std(dim = \"time\")\n",
    "        X2ssp = X2ssp/X2hist.std(dim = \"time\")\n",
    "        X3ssp = xr.where((X3ssp/X3hist.std(dim = \"time\")).fillna(0.001) == np.inf, 0.001, (X3ssp/X3hist.std(dim = \"time\"))).fillna(0.001)     # sometimes it could happen to have Inf values\n",
    "        X4ssp = (X4ssp/X4hist.std(dim = \"time\"))\n",
    "        X5ssp = (X5ssp/X5hist.std(dim = \"time\"))\n",
    "\n",
    "        # Stack on 1D vector\n",
    "        ensossp = ensossp.stack(cell = [\"time\"])\n",
    "        X1ssp = X1ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X2ssp = X2ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X3ssp = X3ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X4ssp = X4ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "        X5ssp = X5ssp.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "        Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "        \n",
    "        # Create Dataarray with ENSO signal in every grid cell to perform regression\n",
    "        Xenso = xr.DataArray(data=None, coords=[X1ssp.time, X1ssp.cell], dims=[\"time\",\"cell\"])\n",
    "        for c in Xenso.cell:\n",
    "            locator = {'cell':c}\n",
    "            Xenso.loc[locator] = ensossp.values\n",
    "            Xenso.values = Xenso.values.astype(\"float64\")       # To avoid the resulting values are \"object\" rather than \"float64\" \n",
    "        Xenso = Xenso.rename(\"sst\")\n",
    "\n",
    "        # Create empty dataarray to store regression coefficients\n",
    "        coef = xr.DataArray(data=None, coords=[X1ssp.time,X1ssp.cell], dims=[\"time\",\"cell\"])\n",
    "        # coef[\"pr\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"tas\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"mrso\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"sst\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"rsds\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        coef[\"vpd\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty dataarray to store p-values\n",
    "        pval = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        # pval[\"pr\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"tas\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"mrso\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"sst\"] = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"rsds\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pval[\"vpd\"] =  xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "\n",
    "        # Create empty datarray to store R2 and Y predicted \n",
    "        r_squared = xr.DataArray(data=None, coords=[X1ssp.cell], dims=[\"cell\"])\n",
    "        pred = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\",\"cell\"])\n",
    "        pred[\"nbp\"] = xr.DataArray(data=None, coords=[Y.time, Y.cell], dims=[\"time\", \"cell\"])\n",
    "\n",
    "        for c in X1ssp.cell:\n",
    "            locator = {'cell':c}\n",
    "            # Check if the mrso value is NaN or 0 for this cell\n",
    "            if np.isnan(res_mrso_ssp[i].stack(cell = [\"lon\",\"lat\"]).loc[locator].values).any() or (res_mrso_ssp[i].stack(cell = [\"lon\",\"lat\"]).loc[locator].values == 0).any():\n",
    "\n",
    "                # Assign NaN to all result arrays for this cell\n",
    "                coef[\"tas\"].loc[locator] = np.nan\n",
    "                coef[\"mrso\"].loc[locator] = np.nan\n",
    "                coef[\"rsds\"].loc[locator] = np.nan\n",
    "                coef[\"vpd\"].loc[locator] = np.nan\n",
    "\n",
    "                pval[\"tas\"].loc[locator] = np.nan\n",
    "                pval[\"mrso\"].loc[locator] = np.nan\n",
    "                pval[\"rsds\"].loc[locator] = np.nan\n",
    "                pval[\"vpd\"].loc[locator] = np.nan\n",
    "\n",
    "                r_squared.loc[locator] = np.nan\n",
    "                pred[\"nbp\"].loc[locator] = np.nan\n",
    "                continue  # Skip this cell and move to the next\n",
    "\n",
    "            # merge predictors in one dataarray\n",
    "            df = np.array((X2ssp.loc[locator].values, X3ssp.loc[locator].values, X4ssp.loc[locator].values, X5ssp.loc[locator].values,Xenso.loc[locator].values)); df = df.T\n",
    "            model = ridge.fit(df,Y.loc[locator].values)\n",
    "            r2 = ridge.score(df,Y.loc[locator])\n",
    "            ypred = ridge.predict(df)\n",
    "\n",
    "            # coef[\"pr\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"tas\"].loc[locator] = model.coef_[0]\n",
    "            coef[\"mrso\"].loc[locator] = model.coef_[1]\n",
    "            coef[\"rsds\"].loc[locator] = model.coef_[2]\n",
    "            coef[\"vpd\"].loc[locator] = model.coef_[3]\n",
    "            coef[\"sst\"].loc[locator] = model.coef_[4]\n",
    "\n",
    "            # p-value of the index \"0\" refers to the intercept\n",
    "            # pval[\"pr\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"tas\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[1]\n",
    "            pval[\"mrso\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[2]\n",
    "            pval[\"rsds\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[3]\n",
    "            pval[\"vpd\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[4]\n",
    "            pval[\"sst\"].loc[locator] = stats.coef_pval(model,df,Y.loc[locator].values)[5]\n",
    "            \n",
    "            r_squared.loc[locator] = r2\n",
    "            pred[\"nbp\"].loc[locator] = ypred\n",
    "\n",
    "        coef = coef.unstack()\n",
    "        coef = xr.concat([coef.tas, coef.mrso, coef.rsds, coef.vpd, coef.sst], dim = \"coefficients\")\n",
    "        # coef[\"pr\"] = coef.pr.astype(np.float64)\n",
    "        coef[\"tas\"] = coef.tas.astype(np.float64)\n",
    "        coef[\"mrso\"] = coef.mrso.astype(np.float64)\n",
    "        coef[\"rsds\"] = coef.rsds.astype(np.float64)\n",
    "        coef[\"vpd\"] = coef.vpd.astype(np.float64)\n",
    "        coef[\"sst\"] = coef.sst.astype(np.float64)\n",
    "\n",
    "        pval = pval.unstack()\n",
    "        pval = xr.concat([pval.tas, pval.mrso, pval.rsds, pval.vpd, pval.sst], dim = \"p-values\")\n",
    "        # pval[\"pr\"] = pval.pr.astype(np.float64)\n",
    "        pval[\"tas\"] = pval.tas.astype(np.float64)\n",
    "        pval[\"mrso\"] = pval.mrso.astype(np.float64)\n",
    "        pval[\"rsds\"] = pval.rsds.astype(np.float64)\n",
    "        pval[\"vpd\"] = pval.vpd.astype(np.float64)\n",
    "        pval[\"sst\"] = pval.sst.astype(np.float64)\n",
    "        \n",
    "        r_squared = r_squared.astype(np.float64).unstack()\n",
    "        pred = pred.astype(np.float64).unstack()\n",
    "\n",
    "        coef_ridge_ssp.append(coef)\n",
    "        r2_ridge_ssp.append(r_squared)\n",
    "        pred_ridge_ssp.append(pred)\n",
    "        pval_ridge_ssp.append(pval)\n",
    "        \n",
    "    # else:\n",
    "    #     None\n",
    "    \n",
    "\n",
    "# Save and export regression list data\n",
    "import pickle\n",
    "data_path = 'G:/My Drive/Amazon_CMIP6/results/'\n",
    "\n",
    "with open(os.path.join(data_path+\"gamma_iav_hist_coef\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(coef_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"gamma_iav_ssp_coef\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(coef_ridge_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"gamma_iav_hist_pval\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"gamma_iav_ssp_pval\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_ridge_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"gamma_iav_hist_r2\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(r2_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"gamma_iav_ssp_r2\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(r2_ridge_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"gamma_iav_hist_ypred\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pred_ridge_hist, fp)\n",
    "with open(os.path.join(data_path+\"gamma_iav_ssp_ypred\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pred_ridge_ssp, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;vpd&#x27; (time: 165, cell: 1056)&gt;\n",
       "array([[-0.56341338, -1.13701474, -1.56364053, ..., -0.6286069 ,\n",
       "        -0.10685729, -0.10181315],\n",
       "       [ 0.19789259, -0.14881061,  0.24232064, ..., -1.54371648,\n",
       "        -1.83661314, -1.68742905],\n",
       "       [-1.52031731, -1.61841485, -1.74482455, ..., -0.9336126 ,\n",
       "        -0.97444539, -1.00883339],\n",
       "       ...,\n",
       "       [-0.85810004, -1.41237936, -1.70740806, ..., -0.2886959 ,\n",
       "        -0.34611555, -0.58575839],\n",
       "       [-0.37137885, -0.37733922, -0.79129979, ...,  0.22062479,\n",
       "         0.40287981,  0.29713288],\n",
       "       [ 1.47484607,  1.54408747,  0.9454899 , ..., -0.75503888,\n",
       "        -0.17223527,  0.39756604]])\n",
       "Coordinates:\n",
       "    height   float64 2.0\n",
       "  * time     (time) datetime64[ns] 1849-12-31 1850-12-31 ... 2013-12-31\n",
       "    plev     float64 9.25e+04\n",
       "  * cell     (cell) object MultiIndex\n",
       "  * lon      (cell) float64 260.0 260.0 260.0 260.0 ... 340.0 340.0 340.0 340.0\n",
       "  * lat      (cell) float64 -29.37 -27.47 -25.58 -23.68 ... 25.58 27.47 29.37</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'vpd'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 165</li><li><span class='xr-has-index'>cell</span>: 1056</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-bd2b696a-871d-435d-8d8e-e2050160c701' class='xr-array-in' type='checkbox' checked><label for='section-bd2b696a-871d-435d-8d8e-e2050160c701' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>-0.5634 -1.137 -1.564 -1.834 -1.728 ... -1.215 -0.755 -0.1722 0.3976</span></div><div class='xr-array-data'><pre>array([[-0.56341338, -1.13701474, -1.56364053, ..., -0.6286069 ,\n",
       "        -0.10685729, -0.10181315],\n",
       "       [ 0.19789259, -0.14881061,  0.24232064, ..., -1.54371648,\n",
       "        -1.83661314, -1.68742905],\n",
       "       [-1.52031731, -1.61841485, -1.74482455, ..., -0.9336126 ,\n",
       "        -0.97444539, -1.00883339],\n",
       "       ...,\n",
       "       [-0.85810004, -1.41237936, -1.70740806, ..., -0.2886959 ,\n",
       "        -0.34611555, -0.58575839],\n",
       "       [-0.37137885, -0.37733922, -0.79129979, ...,  0.22062479,\n",
       "         0.40287981,  0.29713288],\n",
       "       [ 1.47484607,  1.54408747,  0.9454899 , ..., -0.75503888,\n",
       "        -0.17223527,  0.39756604]])</pre></div></div></li><li class='xr-section-item'><input id='section-62232586-9387-46b0-885c-ad94e5765e28' class='xr-section-summary-in' type='checkbox'  checked><label for='section-62232586-9387-46b0-885c-ad94e5765e28' class='xr-section-summary' >Coordinates: <span>(6)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>height</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>2.0</div><input id='attrs-2f7af20a-d1d6-46aa-ae22-0fd4cdbdc767' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2f7af20a-d1d6-46aa-ae22-0fd4cdbdc767' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3685e499-823d-4ce6-befb-99bd6382f06e' class='xr-var-data-in' type='checkbox'><label for='data-3685e499-823d-4ce6-befb-99bd6382f06e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>height</dd><dt><span>long_name :</span></dt><dd>height</dd><dt><span>units :</span></dt><dd>m</dd><dt><span>positive :</span></dt><dd>up</dd><dt><span>axis :</span></dt><dd>Z</dd></dl></div><div class='xr-var-data'><pre>array(2.)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1849-12-31 ... 2013-12-31</div><input id='attrs-ec309a6d-f410-4a39-820d-08770cf4a5b9' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ec309a6d-f410-4a39-820d-08770cf4a5b9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-0c88ff36-c827-41eb-b7ea-519cb3008df0' class='xr-var-data-in' type='checkbox'><label for='data-0c88ff36-c827-41eb-b7ea-519cb3008df0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;1849-12-31T00:00:00.000000000&#x27;, &#x27;1850-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1851-12-31T00:00:00.000000000&#x27;, &#x27;1852-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1853-12-31T00:00:00.000000000&#x27;, &#x27;1854-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1855-12-31T00:00:00.000000000&#x27;, &#x27;1856-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1857-12-31T00:00:00.000000000&#x27;, &#x27;1858-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1859-12-31T00:00:00.000000000&#x27;, &#x27;1860-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1861-12-31T00:00:00.000000000&#x27;, &#x27;1862-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1863-12-31T00:00:00.000000000&#x27;, &#x27;1864-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1865-12-31T00:00:00.000000000&#x27;, &#x27;1866-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1867-12-31T00:00:00.000000000&#x27;, &#x27;1868-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1869-12-31T00:00:00.000000000&#x27;, &#x27;1870-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1871-12-31T00:00:00.000000000&#x27;, &#x27;1872-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1873-12-31T00:00:00.000000000&#x27;, &#x27;1874-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1875-12-31T00:00:00.000000000&#x27;, &#x27;1876-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1877-12-31T00:00:00.000000000&#x27;, &#x27;1878-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1879-12-31T00:00:00.000000000&#x27;, &#x27;1880-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1881-12-31T00:00:00.000000000&#x27;, &#x27;1882-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1883-12-31T00:00:00.000000000&#x27;, &#x27;1884-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1885-12-31T00:00:00.000000000&#x27;, &#x27;1886-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1887-12-31T00:00:00.000000000&#x27;, &#x27;1888-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1889-12-31T00:00:00.000000000&#x27;, &#x27;1890-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1891-12-31T00:00:00.000000000&#x27;, &#x27;1892-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1893-12-31T00:00:00.000000000&#x27;, &#x27;1894-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1895-12-31T00:00:00.000000000&#x27;, &#x27;1896-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1897-12-31T00:00:00.000000000&#x27;, &#x27;1898-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1899-12-31T00:00:00.000000000&#x27;, &#x27;1900-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1901-12-31T00:00:00.000000000&#x27;, &#x27;1902-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1903-12-31T00:00:00.000000000&#x27;, &#x27;1904-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1905-12-31T00:00:00.000000000&#x27;, &#x27;1906-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1907-12-31T00:00:00.000000000&#x27;, &#x27;1908-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1909-12-31T00:00:00.000000000&#x27;, &#x27;1910-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1911-12-31T00:00:00.000000000&#x27;, &#x27;1912-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1913-12-31T00:00:00.000000000&#x27;, &#x27;1914-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1915-12-31T00:00:00.000000000&#x27;, &#x27;1916-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1917-12-31T00:00:00.000000000&#x27;, &#x27;1918-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1919-12-31T00:00:00.000000000&#x27;, &#x27;1920-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1921-12-31T00:00:00.000000000&#x27;, &#x27;1922-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1923-12-31T00:00:00.000000000&#x27;, &#x27;1924-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1925-12-31T00:00:00.000000000&#x27;, &#x27;1926-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1927-12-31T00:00:00.000000000&#x27;, &#x27;1928-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1929-12-31T00:00:00.000000000&#x27;, &#x27;1930-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1931-12-31T00:00:00.000000000&#x27;, &#x27;1932-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1933-12-31T00:00:00.000000000&#x27;, &#x27;1934-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1935-12-31T00:00:00.000000000&#x27;, &#x27;1936-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1937-12-31T00:00:00.000000000&#x27;, &#x27;1938-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1939-12-31T00:00:00.000000000&#x27;, &#x27;1940-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1941-12-31T00:00:00.000000000&#x27;, &#x27;1942-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1943-12-31T00:00:00.000000000&#x27;, &#x27;1944-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1945-12-31T00:00:00.000000000&#x27;, &#x27;1946-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1947-12-31T00:00:00.000000000&#x27;, &#x27;1948-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1949-12-31T00:00:00.000000000&#x27;, &#x27;1950-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1951-12-31T00:00:00.000000000&#x27;, &#x27;1952-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1953-12-31T00:00:00.000000000&#x27;, &#x27;1954-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1955-12-31T00:00:00.000000000&#x27;, &#x27;1956-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1957-12-31T00:00:00.000000000&#x27;, &#x27;1958-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1959-12-31T00:00:00.000000000&#x27;, &#x27;1960-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1961-12-31T00:00:00.000000000&#x27;, &#x27;1962-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1963-12-31T00:00:00.000000000&#x27;, &#x27;1964-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1965-12-31T00:00:00.000000000&#x27;, &#x27;1966-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1967-12-31T00:00:00.000000000&#x27;, &#x27;1968-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1969-12-31T00:00:00.000000000&#x27;, &#x27;1970-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1971-12-31T00:00:00.000000000&#x27;, &#x27;1972-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1973-12-31T00:00:00.000000000&#x27;, &#x27;1974-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1975-12-31T00:00:00.000000000&#x27;, &#x27;1976-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1977-12-31T00:00:00.000000000&#x27;, &#x27;1978-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1979-12-31T00:00:00.000000000&#x27;, &#x27;1980-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-12-31T00:00:00.000000000&#x27;, &#x27;1982-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1983-12-31T00:00:00.000000000&#x27;, &#x27;1984-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1985-12-31T00:00:00.000000000&#x27;, &#x27;1986-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1987-12-31T00:00:00.000000000&#x27;, &#x27;1988-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1989-12-31T00:00:00.000000000&#x27;, &#x27;1990-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1991-12-31T00:00:00.000000000&#x27;, &#x27;1992-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1993-12-31T00:00:00.000000000&#x27;, &#x27;1994-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1995-12-31T00:00:00.000000000&#x27;, &#x27;1996-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1997-12-31T00:00:00.000000000&#x27;, &#x27;1998-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;1999-12-31T00:00:00.000000000&#x27;, &#x27;2000-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2001-12-31T00:00:00.000000000&#x27;, &#x27;2002-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2003-12-31T00:00:00.000000000&#x27;, &#x27;2004-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2005-12-31T00:00:00.000000000&#x27;, &#x27;2006-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2007-12-31T00:00:00.000000000&#x27;, &#x27;2008-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2009-12-31T00:00:00.000000000&#x27;, &#x27;2010-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2011-12-31T00:00:00.000000000&#x27;, &#x27;2012-12-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2013-12-31T00:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>plev</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>9.25e+04</div><input id='attrs-a072471c-58f7-4420-9e3b-f51297fda389' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-a072471c-58f7-4420-9e3b-f51297fda389' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-eb510ffd-ec0b-4587-9582-250b54a9a0b5' class='xr-var-data-in' type='checkbox'><label for='data-eb510ffd-ec0b-4587-9582-250b54a9a0b5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>air_pressure</dd><dt><span>long_name :</span></dt><dd>pressure</dd><dt><span>units :</span></dt><dd>Pa</dd><dt><span>positive :</span></dt><dd>down</dd><dt><span>axis :</span></dt><dd>Z</dd></dl></div><div class='xr-var-data'><pre>array(92500.)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>cell</span></div><div class='xr-var-dims'>(cell)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>MultiIndex</div><input id='attrs-fa81ff8b-b781-4356-ad37-92a926b9ffca' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-fa81ff8b-b781-4356-ad37-92a926b9ffca' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5469a3b8-0eed-4382-a47a-54b12bd62751' class='xr-var-data-in' type='checkbox'><label for='data-5469a3b8-0eed-4382-a47a-54b12bd62751' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([(260.0, -29.3684210526317), (260.0, -27.4736842105264),\n",
       "       (260.0, -25.5789473684211), ..., (340.0, 25.578947368421),\n",
       "       (340.0, 27.4736842105262), (340.0, 29.368421052631504)], dtype=object)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(cell)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>260.0 260.0 260.0 ... 340.0 340.0</div><input id='attrs-bd769e70-c126-4231-a4b2-ec1d4c641cf6' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bd769e70-c126-4231-a4b2-ec1d4c641cf6' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-26a7704b-9061-4697-93e6-31a2ae5fc47b' class='xr-var-data-in' type='checkbox'><label for='data-26a7704b-9061-4697-93e6-31a2ae5fc47b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>Longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>axis :</span></dt><dd>X</dd><dt><span>bounds :</span></dt><dd>lon_bnds</dd></dl></div><div class='xr-var-data'><pre>array([260., 260., 260., ..., 340., 340., 340.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(cell)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-29.37 -27.47 ... 27.47 29.37</div><input id='attrs-3b4be401-e0cc-4b7c-b306-b1a56f30c11d' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3b4be401-e0cc-4b7c-b306-b1a56f30c11d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e2257ac8-b63c-4ac2-9e04-8df85aa2af6b' class='xr-var-data-in' type='checkbox'><label for='data-e2257ac8-b63c-4ac2-9e04-8df85aa2af6b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>Latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>axis :</span></dt><dd>Y</dd><dt><span>bounds :</span></dt><dd>lat_bnds</dd></dl></div><div class='xr-var-data'><pre>array([-29.368421, -27.473684, -25.578947, ...,  25.578947,  27.473684,\n",
       "        29.368421])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-2adfca6a-cac8-4cb4-9f1f-a51601ce24f5' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-2adfca6a-cac8-4cb4-9f1f-a51601ce24f5' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'vpd' (time: 165, cell: 1056)>\n",
       "array([[-0.56341338, -1.13701474, -1.56364053, ..., -0.6286069 ,\n",
       "        -0.10685729, -0.10181315],\n",
       "       [ 0.19789259, -0.14881061,  0.24232064, ..., -1.54371648,\n",
       "        -1.83661314, -1.68742905],\n",
       "       [-1.52031731, -1.61841485, -1.74482455, ..., -0.9336126 ,\n",
       "        -0.97444539, -1.00883339],\n",
       "       ...,\n",
       "       [-0.85810004, -1.41237936, -1.70740806, ..., -0.2886959 ,\n",
       "        -0.34611555, -0.58575839],\n",
       "       [-0.37137885, -0.37733922, -0.79129979, ...,  0.22062479,\n",
       "         0.40287981,  0.29713288],\n",
       "       [ 1.47484607,  1.54408747,  0.9454899 , ..., -0.75503888,\n",
       "        -0.17223527,  0.39756604]])\n",
       "Coordinates:\n",
       "    height   float64 2.0\n",
       "  * time     (time) datetime64[ns] 1849-12-31 1850-12-31 ... 2013-12-31\n",
       "    plev     float64 9.25e+04\n",
       "  * cell     (cell) object MultiIndex\n",
       "  * lon      (cell) float64 260.0 260.0 260.0 260.0 ... 340.0 340.0 340.0 340.0\n",
       "  * lat      (cell) float64 -29.37 -27.47 -25.58 -23.68 ... 25.58 27.47 29.37"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X5hist.isel(plev = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open regression list data\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_path = 'G:/Shared drives/Amazon_ENSO_work/analysis/'\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_coef_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    coef_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_coef_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    coef_ridge_ssp = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_pval_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pval_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_pval_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pval_ridge_ssp = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_r2_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    r2_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_r2_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    r2_ridge_ssp = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_hist_1901-1960_DJF_ypred_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pred_ridge_hist = pickle.load(fp)\n",
    "with open(os.path.join(data_path+\"ridge_reg_enso_ssp_2041-2100_DJF_ypred_c_new\"), \"rb\") as fp:   #Pickling\n",
    "    pred_ridge_ssp = pickle.load(fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# stack variables over lon-lat dimensions\n",
    "# Xenso = nino34_hist[0].rename(\"sst\")#; Xenso = Xenso.stack(cell = [\"time\"])\n",
    "X1 = ds_hist_pr[0].rename(\"pr\").sel(lon= slice(260,340))\n",
    "X2 = ds_hist_tas[0].rename(\"tas\").sel(lon= slice(260,340))\n",
    "X3 = ds_hist_mrso[0].rename(\"mrso\").sel(lon= slice(260,340), lat = slice(-30,30)).fillna(0)\n",
    "Y = ds_hist_nbp[0].sel(lon= slice(260,340), lat = slice(-30,30)).fillna(0)\n",
    "\n",
    "# Standardize predictors\n",
    "X1 = X1/X1.std(dim = \"time\")\n",
    "X2 = X2/X2.std(dim = \"time\")\n",
    "X3 = (X3/X3.std(dim = \"time\")).fillna(0)\n",
    "\n",
    "# Stack on 1D vector\n",
    "X1 = X1.stack(cell = [\"lon\",\"lat\"])\n",
    "X2 = X2.stack(cell = [\"lon\",\"lat\"])\n",
    "X3 = X3.stack(cell = [\"lon\",\"lat\"])\n",
    "Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "# test with ENSO signal in every grid cell\n",
    "# test = xr.DataArray(data=None, coords=[X1.time, X1.cell], dims=[\"time\",\"cell\"])\n",
    "# for i in test.cell:\n",
    "#     locator = {'cell':i}\n",
    "#     test.loc[locator] = Xenso.values\n",
    "# test = test.rename(\"sst\")\n",
    "\n",
    "# merge predictors in one dataarray\n",
    "X = xr.merge([X1, X2, X3]).to_array().transpose(\"time\", \"variable\", \"cell\")\n",
    "\n",
    "# Define RF parameters\n",
    "rf_regr = RandomForestRegressor(n_estimators=100,\n",
    "                              criterion = \"squared_error\",\n",
    "                              max_depth=10,\n",
    "                              max_samples=0.7,\n",
    "                              random_state=0)\n",
    "\n",
    "# Create empty dataarray to store regression coefficients\n",
    "coef = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "coef[\"pr\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "coef[\"tas\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "coef[\"mrso\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "# coef[\"sst\"] =  xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "\n",
    "for i in X.cell:\n",
    "    locator = {'cell':i}\n",
    "    model = rf_regr.fit(X.loc[locator],Y.loc[locator])\n",
    "    coef[\"pr\"].loc[locator] = model.feature_importances_[0]\n",
    "    coef[\"tas\"].loc[locator] = model.feature_importances_[1]\n",
    "    coef[\"mrso\"].loc[locator] = model.feature_importances_[2]\n",
    "\n",
    "coef = coef.unstack()\n",
    "pr_ridge = coef.pr.astype(np.float64)\n",
    "tas_ridge = coef.tas.astype(np.float64)\n",
    "mrso_ridge = coef.mrso.astype(np.float64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_clim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
