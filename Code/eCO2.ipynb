{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = r'C:/Users/mastr/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Windows OS\n",
    "# os.environ['PROJ_LIB'] = r'/Users/mmastro/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Mac OS\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#from scipy.signal import argrelextrema                      # Find local Maxima-Minima in numpy array\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_dim(da, dim, degree):\n",
    "    # Store original attributes\n",
    "    original_attrs = da.attrs\n",
    "\n",
    "    # Detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=degree)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    da_det = da - fit\n",
    "    \n",
    "    # Restore original attributes\n",
    "    da_det.attrs = original_attrs\n",
    "    \n",
    "    return da_det\n",
    "\n",
    "def xr_mean_list(xarray_list):\n",
    "    # Step 1: Group the xarray objects by their \"esm\" attribute\n",
    "    esm_groups = {}\n",
    "    for xr_obj in xarray_list:\n",
    "        esm_value = xr_obj.attrs.get('esm', None)\n",
    "        if esm_value not in esm_groups:\n",
    "            esm_groups[esm_value] = []\n",
    "        esm_groups[esm_value].append(xr_obj)\n",
    "\n",
    "    # Step 2: Sort the esm groups alphabetically by their esm value\n",
    "    sorted_esm_values = sorted(esm_groups.keys())\n",
    "\n",
    "    # Step 3: Calculate the mean for each group in alphabetical order\n",
    "    mean_results = {}\n",
    "    for esm_value in sorted_esm_values:\n",
    "        xr_objs = esm_groups[esm_value]\n",
    "        \n",
    "        # Concatenate all xarray objects in this group along a new dimension (e.g., 'stacked_xarrays')\n",
    "        combined = xr.concat(xr_objs, dim='stacked_xarrays')\n",
    "        \n",
    "        # Calculate the mean along the 'stacked_xarrays' dimension\n",
    "        mean_results[esm_value] = combined.mean(dim='stacked_xarrays')\n",
    "    \n",
    "    return mean_results\n",
    "\n",
    "\n",
    "## Fuction for subsetting colormap values ## \n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "## Function for creating a path, if needed ##\n",
    "def checkDir(out_path):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import t\n",
    "\n",
    "def lag_linregress_3D(x, y, lagx=0, lagy=0):\n",
    "    \"\"\"\n",
    "    Input: Two xr.DataArrays of any dimensions with the first dim being time. \n",
    "    Thus the input data could be a 1D time series, or for example, have three dimensions (time,lat,lon). \n",
    "    Datasets can be provided in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    Output: Covariance, correlation, regression slope and intercept, p-value, standard error on regression,\n",
    "    and R-squared between the two datasets along their aligned time dimension.  \n",
    "    Lag values can be assigned to either of the data, with lagx shifting x, and lagy shifting y, with the specified lag amount. \n",
    "    \"\"\" \n",
    "    # 1. Ensure that the data are properly aligned to each other. \n",
    "    x, y = xr.align(x, y)\n",
    "    \n",
    "    # 2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx != 0:\n",
    "        x = x.shift(time=-lagx).dropna(dim='time')\n",
    "        x, y = xr.align(x, y)\n",
    "\n",
    "    if lagy != 0:\n",
    "        y = y.shift(time=-lagy).dropna(dim='time')\n",
    "        x, y = xr.align(x, y)\n",
    " \n",
    "    # 3. Compute data length, mean and standard deviation along time axis for further use\n",
    "    n = x.shape[0]\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd = x.std(axis=0)\n",
    "    ystd = y.std(axis=0)\n",
    "    \n",
    "    # 4. Compute covariance along time axis\n",
    "    cov = np.sum((x - xmean) * (y - ymean), axis=0) / n\n",
    "    \n",
    "    # 5. Compute correlation along time axis\n",
    "    cor = cov / (xstd * ystd)\n",
    "    \n",
    "    # 6. Compute regression slope and intercept\n",
    "    slope = cov / (xstd ** 2)\n",
    "    intercept = ymean - xmean * slope\n",
    "    y_pred = intercept + slope * x\n",
    "    res = y - y_pred\n",
    "\n",
    "    # 7. Compute P-value and standard error\n",
    "    # Compute t-statistics\n",
    "    tstats = cor * np.sqrt(n-2) / np.sqrt(1 - cor**2)\n",
    "    stderr = slope / tstats\n",
    "    \n",
    "    pval = t.sf(tstats, n-2) * 2\n",
    "    pval = xr.DataArray(pval, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    # 8. Compute R-squared\n",
    "    sst = np.sum((y - ymean) ** 2, axis=0)\n",
    "    ssr = np.sum(res ** 2, axis=0)\n",
    "    r_squared = 1 - ssr / sst\n",
    "\n",
    "    r_squared = xr.DataArray(r_squared, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    return slope #, intercept, pval, stderr, r_squared, y_pred\n",
    "\n",
    "\n",
    "def xr_multipletest(p, alpha=0.05, method='fdr_bh', **multipletests_kwargs):\n",
    "    \"\"\"Apply statsmodels.stats.multitest.multipletests for multi-dimensional xr.objects.\"\"\"\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    # stack all to 1d array\n",
    "    p_stacked = p.stack(s=p.dims)\n",
    "    # mask only where not nan: https://github.com/statsmodels/statsmodels/issues/2899\n",
    "    mask = np.isfinite(p_stacked)\n",
    "    pvals_corrected = np.full(p_stacked.shape, np.nan)\n",
    "    reject = np.full(p_stacked.shape, np.nan)\n",
    "    # apply test where mask\n",
    "    reject[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[0]\n",
    "    pvals_corrected[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[1]\n",
    "\n",
    "    def unstack(reject, p_stacked):\n",
    "        \"\"\"Exchange values from p_stacked with reject (1d array) and unstack.\"\"\"\n",
    "        xreject = p_stacked.copy()\n",
    "        xreject.values = reject\n",
    "        xreject = xreject.unstack()\n",
    "        return xreject\n",
    "\n",
    "    reject = unstack(reject, p_stacked)\n",
    "    pvals_corrected = unstack(pvals_corrected, p_stacked)\n",
    "    return reject, pvals_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zonal mean of REAN and ESM\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "from importlib.machinery import SourceFileLoader\n",
    "# imports the module from the given path\n",
    "gpd = SourceFileLoader(\"geopandas\",\"C:/Users/mastr/miniconda3/pkgs/geopandas-0.7.0-py_1/site-packages/geopandas/__init__.py\").load_module()\n",
    "import regionmask\n",
    "\n",
    "def cell_weight(ds):\n",
    "    R = 6.371e6\n",
    "    dϕ = np.deg2rad(ds.lat[1] - ds.lat[0])\n",
    "    dλ = np.deg2rad(ds.lon[1] - ds.lon[0])\n",
    "    dlat = R * dϕ * xr.ones_like(ds.lon)\n",
    "    dlon = R * dλ * np.cos(np.deg2rad(ds.lat))\n",
    "    cell_area = dlon * dlat\n",
    "    return(cell_area)\n",
    "\n",
    "def mask_shape(ds,gdf):\n",
    "    # Create mask of multiple regions from shapefile\n",
    "    mask = regionmask.mask_3D_geopandas(\n",
    "            gdf,\n",
    "            ds.lon,\n",
    "            ds.lat,\n",
    "            drop=True\n",
    "        )\n",
    "    ds_m = ds.where(mask)\n",
    "    return ds_m\n",
    "\n",
    "def zonal_stat(ds,gdf):\n",
    "    # Create mask of multiple regions from shapefile\n",
    "    mask = regionmask.mask_3D_geopandas(\n",
    "            gdf,\n",
    "            ds.lon,\n",
    "            ds.lat,\n",
    "            drop=True\n",
    "        )\n",
    "    # Apply mask to xarray\n",
    "    ds_m = ds.where(mask)\n",
    "    # Calculate cell area of the xarray\n",
    "    cell_area = cell_weight(ds)\n",
    "    # Zonal statistic for all the regions\n",
    "    ds_z = ds_m.weighted(cell_area).mean([\"lat\",\"lon\"])\n",
    "    # ds_z = ds_z.groupby(\"region\").mean(])\n",
    "    return ds_z\n",
    "\n",
    "# geodf = gpd.read_file((os.path.join('C:/Users/mastr/Documents/Amazon/amazon_border/biome.shp')))\n",
    "geodf = gpd.read_file((os.path.join('C:/Users/mastr/Documents/Amazon/hybas_sa_lev01-12_v1c/hybas_sa_lev02_v1c.shp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Path for the output files (images, etc)\n",
    "# out_path = 'C:/Users/mastr/Documents/Amazon/RESULTS/'\n",
    "out_path = 'G:/Shared drives/Amazon_ENSO_work/RESULTS/MLR/'\n",
    "\n",
    "# -- Create directories\n",
    "checkDir(out_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open 1pctCO2-bgc data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'\n",
    "\n",
    "scenario = '1pctCO2-bgc'\n",
    "var_name = 'nep'\n",
    "files = var_name + '_*_' + scenario + '.nc' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)                                              # to retrieve netcdf original ATTRIBUT\n",
    "        \n",
    "ds_nep = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          #, chunks = {\"time\" : 720}\n",
    "    ds_nep.append(content)\n",
    "\n",
    "# Retain from year 1850 to 1989\n",
    "# Uniform calendar, assign esm name, subsetting latitude, resample from M to Y\n",
    "for i, item in enumerate(ds_nep):\n",
    "    ds_nep[i] = ds_nep[i].isel(time = slice(0,1680))\n",
    "    ds_nep[i] = ds_nep[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    ds_nep[i]['time'] = pd.date_range(start = \"1850\", periods=1680, freq = \"M\")\n",
    "    ds_nep[i] = ds_nep[i].sel(lat=slice(-30,30), lon= slice(260,340))\n",
    "    ds_nep[i] = ds_nep[i].resample(time=\"Y\", label='right').mean()\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_nep[i] = ds_nep[i].where(ds_nep[i] != -3.7270379e-07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "CESM2\n",
      "CMCC-ESM2\n",
      "CNRM-ESM2-1\n",
      "CanESM5\n",
      "IPSL-CM6A-LR\n",
      "MIROC-ES2L\n",
      "MPI-ESM1-2-LR\n",
      "MRI-ESM2-0\n",
      "NorESM2-LM\n",
      "UKESM1-0-LL\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(ds_nep):\n",
    "    if item.esm in {\"BCC-CSM2-MR\",\"E3SM-1-1-ECA\",\"TaiESM1\"}: #,\"MRI-ESM2-0\", \"NorESM2-MM\"\n",
    "        del ds_nep[i]\n",
    "    print(ds_nep[i].esm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/Data/analysis/'\n",
    "\n",
    "scenario = '1pctCO2-bgc'\n",
    "var_name = 'nbp'\n",
    "files = var_name + '_*_' + scenario + '.nc' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)                                              # to retrieve netcdf original ATTRIBUT\n",
    "        \n",
    "ds_nbp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          #, chunks = {\"time\" : 720}\n",
    "    ds_nbp.append(content)\n",
    "\n",
    "# Retain from year 1850 to 1989\n",
    "# Uniform calendar, assign esm name, subsetting latitude, resample from M to Y\n",
    "for i, item in enumerate(ds_nbp):\n",
    "    ds_nbp[i] = ds_nbp[i].isel(time = slice(0,1680))\n",
    "    ds_nbp[i] = ds_nbp[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    ds_nbp[i]['time'] = pd.date_range(start = \"1850\", periods=1680, freq = \"M\")\n",
    "    ds_nbp[i] = ds_nbp[i].sel(lat=slice(-30,30), lon= slice(260,340))\n",
    "    ds_nbp[i] = ds_nbp[i].resample(time=\"Y\", label='right').mean()\n",
    "\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_nbp[i] = ds_nbp[i].where(ds_nbp[i] != -3.7270379e-07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "CESM2\n",
      "CMCC-ESM2\n",
      "CNRM-ESM2-1\n",
      "CanESM5\n",
      "IPSL-CM6A-LR\n",
      "MIROC-ES2L\n",
      "MPI-ESM1-2-LR\n",
      "MRI-ESM2-0\n",
      "NorESM2-LM\n",
      "UKESM1-0-LL\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(ds_nbp):\n",
    "    if item.esm in {\"BCC-CSM2-MR\",\"E3SM-1-1-ECA\",\"TaiESM1\"}: #\"MRI-ESM2-0\", \"NorESM2-MM\"\n",
    "        del ds_nbp[i]\n",
    "    print(ds_nbp[i].esm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = \"nep\"\n",
    "scenario = 'historical'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "ds_hist_nep = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_hist_nep.append(content)    \n",
    "\n",
    "for i, item in enumerate(ds_hist_nep):    \n",
    "    ds_hist_nep[i] = ds_hist_nep[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    ds_hist_nep[i] = ds_hist_nep[i].sel(time = slice(\"1850-01\", \"2014-12\"))\n",
    "    ds_hist_nep[i] = ds_hist_nep[i].sel(lat=slice(-30,30), lon= slice(260,340))\n",
    "    #ds_hist_nep[i] = ds_hist_nep[i].resample(time=\"Y\", label='right').mean()\n",
    "    if ds_hist_nep[i]['time'].dt.calendar == 'noleap' or ds_hist_nep[i]['time'].dt.calendar == '360_day':\n",
    "        ds_hist_nep[i]['time'] = ds_hist_nep[i].indexes['time'].to_datetimeindex()\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_nep[i] = ds_hist_nep[i].where(ds_hist_nep[i] != -6.18386321e-06)\n",
    "    if files_list[i].source_id == \"CESM2-WACCM\":\n",
    "        ds_hist_nep[i] = ds_hist_nep[i]*-1\n",
    "        ds_hist_nep[i] = ds_hist_nep[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    if files_list[i].source_id == \"UKESM1-0-LL\":\n",
    "        ds_hist_nep[i][\"time\"] = ds_hist_nep[0][\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hist_nep = [item for item in ds_hist_nep if item.esm not in {\"BCC-CSM2-MR\", \"E3SM-1-1-ECA\", \"TaiESM1\", \"NorESM2-MM\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = \"nbp\"\n",
    "scenario = 'historical'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "ds_hist_nbp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_hist_nbp.append(content)    \n",
    "\n",
    "for i, item in enumerate(ds_hist_nbp):    \n",
    "    ds_hist_nbp[i] = ds_hist_nbp[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    ds_hist_nbp[i] = ds_hist_nbp[i].sel(time = slice(\"1850-01\", \"2014-12\"))\n",
    "    ds_hist_nbp[i] = ds_hist_nbp[i].sel(lat=slice(-30,30), lon= slice(260,340))\n",
    "    #ds_hist_nbp[i] = ds_hist_nbp[i].resample(time=\"Y\", label='right').mean()\n",
    "    if ds_hist_nbp[i]['time'].dt.calendar == 'noleap' or ds_hist_nbp[i]['time'].dt.calendar == '360_day':\n",
    "        ds_hist_nbp[i]['time'] = ds_hist_nbp[i].indexes['time'].to_datetimeindex()\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_hist_nbp[i] = ds_hist_nbp[i].where(ds_hist_nbp[i] != -6.18386321e-06)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hist_nbp = [item for item in ds_hist_nbp if item.esm not in {\"E3SM-1-1-ECA\", \"TaiESM1\", \"NorESM2-MM\"}]\n",
    "files_list = [item for item in files_list if item.source_id not in {\"E3SM-1-1-ECA\", \"TaiESM1\", \"NorESM2-MM\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_hist_nbp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open ssp585 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = \"nep\"\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "ds_ssp_nep = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_nep.append(content)    \n",
    "\n",
    "for i, item in enumerate(ds_ssp_nep):    \n",
    "    ds_ssp_nep[i] = ds_ssp_nep[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    ds_ssp_nep[i] = ds_ssp_nep[i].sel(time = slice(\"2015-01\", \"2100-12\"))\n",
    "    ds_ssp_nep[i] = ds_ssp_nep[i].sel(lat=slice(-30,30), lon= slice(260,340))\n",
    "    #ds_ssp_nep[i] = ds_ssp_nep[i].resample(time=\"Y\", label='right').mean()\n",
    "    if ds_ssp_nep[i]['time'].dt.calendar == 'noleap' or ds_ssp_nep[i]['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_nep[i]['time'] = ds_ssp_nep[i].indexes['time'].to_datetimeindex()\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_ssp_nep[i] = ds_ssp_nep[i].where(ds_ssp_nep[i] != -6.18386321e-06)\n",
    "    if files_list[i].source_id == \"CESM2-WACCM\":\n",
    "        ds_ssp_nep[i] = ds_ssp_nep[i]*-1\n",
    "        ds_ssp_nep[i] = ds_ssp_nep[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    if files_list[i].source_id == \"UKESM1-0-LL\":\n",
    "        ds_ssp_nep[i][\"time\"] = ds_ssp_nep[0][\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ssp_nep = [item for item in ds_ssp_nep if item.esm not in {\"BCC-CSM2-MR\", \"E3SM-1-1-ECA\", \"TaiESM1\", \"NorESM2-MM\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = \"nbp\"\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "ds_ssp_nbp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    ds_ssp_nbp.append(content)    \n",
    "\n",
    "for i, item in enumerate(ds_ssp_nbp):    \n",
    "    ds_ssp_nbp[i] = ds_ssp_nbp[i].assign_attrs(esm=files_list[i].source_id)\n",
    "    ds_ssp_nbp[i] = ds_ssp_nbp[i].sel(time = slice(\"2015-01\", \"2100-12\"))\n",
    "    ds_ssp_nbp[i] = ds_ssp_nbp[i].sel(lat=slice(-30,30), lon= slice(260,340))\n",
    "    #ds_ssp_nbp[i] = ds_ssp_nbp[i].resample(time=\"Y\", label='right').mean()\n",
    "    if ds_ssp_nbp[i]['time'].dt.calendar == 'noleap' or ds_ssp_nbp[i]['time'].dt.calendar == '360_day':\n",
    "        ds_ssp_nbp[i]['time'] = ds_ssp_nbp[i].indexes['time'].to_datetimeindex()\n",
    "\n",
    "# CanESM5 hist and ssp have numerical values of ocean equal to \n",
    "# hist: -9.304011e-07\n",
    "# ssp: -1.120609e-05\n",
    "\n",
    "for i, item in enumerate(files_list):\n",
    "    if files_list[i].source_id == \"CanESM5\":\n",
    "        ds_ssp_nbp[i] = ds_ssp_nbp[i].where(ds_ssp_nbp[i] != -6.18386321e-06)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ssp_nbp = [item for item in ds_ssp_nbp if item.esm not in {\"E3SM-1-1-ECA\", \"TaiESM1\", \"NorESM2-MM\"}]\n",
    "files_list = [item for item in files_list if item.source_id not in {\"E3SM-1-1-ECA\", \"TaiESM1\", \"NorESM2-MM\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define list of models and realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for models and realizations\n",
    "models = []\n",
    "realiz = []\n",
    "\n",
    "# Collect the models and realizations\n",
    "for i, item in enumerate(files_list):\n",
    "    models.append(item.source_id)\n",
    "    realiz.append(item.variant_label)\n",
    "\n",
    "# Zip the models and realiz lists together for sorting by source_id (models)\n",
    "zipped = list(zip(models, realiz))\n",
    "\n",
    "# Sort by the first element in each tuple (i.e., source_id)\n",
    "zipped_sorted = sorted(zipped, key=lambda x: x[0])\n",
    "\n",
    "# Unzip the sorted list back into separate lists for models and realiz\n",
    "models_sorted, realiz_sorted = zip(*zipped_sorted)\n",
    "\n",
    "# Convert them back to lists if needed\n",
    "models_sorted = list(models_sorted)\n",
    "realiz_sorted = list(realiz_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.loadtxt('F:/Data/analysis/1pctCO2-bgc/co2_1pctCO2-bgc.txt')\n",
    "\n",
    "# Extract lon, lat, and data values from the loaded data\n",
    "time = data[:140, 0]\n",
    "values = data[:140, 1]\n",
    "\n",
    "ds_co2 = []\n",
    "for i,item in enumerate(ds_nep):\n",
    "\n",
    "    lons = item.lon.values\n",
    "    lats = item.lat.values\n",
    "\n",
    "    # Create coordinate arrays\n",
    "    time_coords = pd.date_range(start = \"1850\", periods=time.shape[0], freq = \"YS\") # xr.DataArray(time, dims='time', name='time')\n",
    "    lon_coords = xr.DataArray(lons, dims='lon', name='lon')\n",
    "    lat_coords = xr.DataArray(lats, dims='lat', name='lat')\n",
    "\n",
    "    # Create a sample data array with a constant value for all grid cells\n",
    "    constant_value = 10  # Replace with your desired constant value\n",
    "    data_values = np.full((len(time), len(lats), len(lons)), constant_value)\n",
    "\n",
    "    # Create the DataArray\n",
    "    co2 = xr.DataArray(data_values, \n",
    "                            coords={'time': time_coords, 'lat': lats, 'lon': lons}, \n",
    "                            dims=['time', 'lat', 'lon'],\n",
    "                            name='ppm').astype(\"float64\")\n",
    "\n",
    "    for i,item in enumerate(time):\n",
    "        co2[i] = values[i]\n",
    "    ds_co2.append(co2)\n",
    "\n",
    "ds_co2 = [a.sel(lat=slice(-30,30), lon= slice(260,340)) for a in ds_co2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression of NEP and CO2\n",
    "\n",
    "The relationship between NEP and CO2 is logarithimic, but if we consider the range of CO2 ppm of historical (280-400) and ssp585 (400-1135) separately, it is linear.\\\n",
    "For historical, the period is from 1850-01 to 1885-12 (36 yrs). \\\n",
    "For ssp585, the period is from 1886-01-01 to 1989-01-01 (104 yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco2_nep_hist = []\n",
    "eco2_nbp_hist = []\n",
    "ds_co2_hist = ds_co2.copy()\n",
    "\n",
    "for i, item in enumerate(ds_nep):\n",
    "    ds_co2_hist[i] = ds_co2[i].assign_coords(lat = ds_nep[i][:35].lat, lon = ds_nep[i][:35].lon, time = ds_nep[i].time)\n",
    "    slope = lag_linregress_3D(ds_co2[i][:35],ds_nep[i][:35]); slope = slope.assign_attrs(esm = ds_nep[i].esm)\n",
    "\n",
    "    ds_co2_hist[i] = ds_co2[i].assign_coords(lat = ds_nbp[i][:35].lat, lon = ds_nbp[i][:35].lon, time = ds_nbp[i].time)\n",
    "    slope1 = lag_linregress_3D(ds_co2[i][:35],ds_nbp[i][:35]); slope1 = slope1.assign_attrs(esm = ds_nbp[i].esm)\n",
    "\n",
    "    eco2_nep_hist.append(slope)\n",
    "    eco2_nbp_hist.append(slope1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco2_nep = []\n",
    "eco2_nbp = []\n",
    "\n",
    "for i, item in enumerate(ds_nep):\n",
    "    ds_co2[i] = ds_co2[i].assign_coords(lat = ds_nep[i][36:].lat, lon = ds_nep[i][36:].lon, time = ds_nep[i].time)\n",
    "    slope = lag_linregress_3D(ds_co2[i][36:],ds_nep[i][36:]); slope = slope.assign_attrs(esm = ds_nep[i].esm)\n",
    "\n",
    "    ds_co2[i] = ds_co2[i].assign_coords(lat = ds_nbp[i][36:].lat, lon = ds_nbp[i][36:].lon, time = ds_nbp[i].time)\n",
    "    slope1 = lag_linregress_3D(ds_co2[i][36:],ds_nbp[i][36:]); slope1 = slope1.assign_attrs(esm = ds_nbp[i].esm)\n",
    "\n",
    "    eco2_nep.append(slope)\n",
    "    eco2_nbp.append(slope1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to correct the esm attribute, since in 1pctCO2-bgc we have CESM2, but in ssp585 we use CESM2-WACCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco2_nep_hist[1] = eco2_nep_hist[1].assign_attrs(esm = \"CESM2-WACCM\")\n",
    "eco2_nbp_hist[1] = eco2_nbp_hist[1].assign_attrs(esm = \"CESM2-WACCM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco2_nep[1] = eco2_nep[1].assign_attrs(esm = \"CESM2-WACCM\")\n",
    "eco2_nbp[1] = eco2_nbp[1].assign_attrs(esm = \"CESM2-WACCM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate ds_co2_hist ppm to the 2015-2100 period (86 years)\n",
    "#x_old = np.linspace(1, ds_co2_hist[0][:35].time.shape[0], ds_co2_hist[0][:35].time.shape[0])\n",
    "#x_new = np.linspace(1, ds_hist_nep[0].time.shape[0], ds_hist_nep[0].time.shape[0])\n",
    "\n",
    "# Interpolate the values of CO2 and0 predict the NEP_eCO2 \n",
    "co2_new_hist = np.linspace(ds_co2[0][0].isel(lat = 0, lon = 0).values, ds_co2[0][35].isel(lat = 0, lon = 0).values, ds_hist_nep[0].time.shape[0])\n",
    "\n",
    "# Create new CO2 xarray\n",
    "ds_co2_new_hist = []\n",
    "for i,item in enumerate(ds_hist_nep):\n",
    "\n",
    "    lons = item.lon.values\n",
    "    lats = item.lat.values\n",
    "    \n",
    "    # Create coordinate arrays\n",
    "    time_coords = item.time.values # xr.DataArray(time, dims='time', name='time')\n",
    "    lon_coords = xr.DataArray(lons, dims='lon', name='lon')\n",
    "    lat_coords = xr.DataArray(lats, dims='lat', name='lat')\n",
    "\n",
    "    # Create a sample data array with a constant value for all grid cells\n",
    "    constant_value = 10  # Replace with your desired constant value\n",
    "    data_values = np.full((len(time_coords), len(lats), len(lons)), constant_value)\n",
    "\n",
    "    # Create the DataArray\n",
    "    co2 = xr.DataArray(data_values, \n",
    "                            coords={'time': time_coords, 'lat': lats, 'lon': lons}, \n",
    "                            dims=['time', 'lat', 'lon'],\n",
    "                            name='ppm').astype(\"float64\")\n",
    "\n",
    "    for i,item in enumerate(time_coords):\n",
    "        co2[i] = co2_new_hist[i]\n",
    "    ds_co2_new_hist.append(co2)\n",
    "\n",
    "ds_co2_new_hist = [a.sel(lat=slice(-30,30), lon= slice(260,340)) for a in ds_co2_new_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate ds_co2 ppm to the 2015-2100 period (86 years)\n",
    "#x_old = np.linspace(1, ds_co2[0][36:].time.shape[0], ds_co2[0][36:].time.shape[0])\n",
    "#x_new = np.linspace(1, ds_ssp_nep[0].time.shape[0], ds_ssp_nep[0].time.shape[0])\n",
    "\n",
    "# Linearly Interpolate the values of CO2 and predict the NEP_eCO2 \n",
    "co2_new = np.linspace(ds_co2[0][36].isel(lat = 0, lon = 0).values, ds_co2[0][-1].isel(lat = 0, lon = 0).values, ds_ssp_nep[0].time.shape[0])\n",
    "\n",
    "# Create new CO2 xarray\n",
    "ds_co2_new = []\n",
    "for i,item in enumerate(ds_ssp_nep):\n",
    "\n",
    "    lons = item.lon.values\n",
    "    lats = item.lat.values\n",
    "    \n",
    "    # Create coordinate arrays\n",
    "    time_coords = item.time.values # xr.DataArray(time, dims='time', name='time')\n",
    "    lon_coords = xr.DataArray(lons, dims='lon', name='lon')\n",
    "    lat_coords = xr.DataArray(lats, dims='lat', name='lat')\n",
    "\n",
    "    # Create a sample data array with a constant value for all grid cells\n",
    "    constant_value = 10  # Replace with your desired constant value\n",
    "    data_values = np.full((len(time_coords), len(lats), len(lons)), constant_value)\n",
    "\n",
    "    # Create the DataArray\n",
    "    co2 = xr.DataArray(data_values, \n",
    "                            coords={'time': time_coords, 'lat': lats, 'lon': lons}, \n",
    "                            dims=['time', 'lat', 'lon'],\n",
    "                            name='ppm').astype(\"float64\")\n",
    "\n",
    "    for i,item in enumerate(time_coords):\n",
    "        co2[i] = co2_new[i]\n",
    "    ds_co2_new.append(co2)\n",
    "\n",
    "ds_co2_new = [a.sel(lat=slice(-30,30), lon= slice(260,340)) for a in ds_co2_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the eCO2 effect estimated from 1pctCo2-bgc simulations from NEP and NBP in historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hist_nep_eco2 = []; ds_hist_nbp_eco2 = []\n",
    "ds_hist_nep_net = []; ds_hist_nbp_net = []\n",
    "\n",
    "for i, item in enumerate(ds_hist_nep):\n",
    "    esm = item.esm\n",
    "\n",
    "    for e, ee in enumerate(eco2_nep_hist):\n",
    "        if eco2_nep[e].esm == esm:\n",
    "            content = ds_co2_new_hist[i] * eco2_nep_hist[e]\n",
    "            content1 = ds_co2_new_hist[i] * eco2_nbp_hist[e]\n",
    "            break\n",
    "        \n",
    "    ds_hist_nep_eco2.append(content.assign_attrs(esm = item.esm))\n",
    "    ds_hist_nbp_eco2.append(content1.assign_attrs(esm = item.esm))\n",
    "\n",
    "    net_nep = (ds_hist_nep[i] - ds_hist_nep_eco2[-1]); net_nep = net_nep.assign_attrs(esm = item.esm)\n",
    "    ds_hist_nep_net.append(net_nep)\n",
    "\n",
    "    net_nbp = (ds_hist_nbp[i] - ds_hist_nbp_eco2[-1]); net_nbp = net_nbp.assign_attrs(esm = item.esm)\n",
    "    ds_hist_nbp_net.append(net_nbp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Export files \n",
    "for i,item in enumerate(ds_hist_nep_net):\n",
    "\n",
    "    ds_hist_nep_eco2[i].to_netcdf(os.path.join(data_path+\"/historical-rad/\"+\"nep_\"+item.esm+\"_\"+\"historical\"+\"_\"+realiz_sorted[i]+\"_eco2.nc\"))\n",
    "    ds_hist_nbp_eco2[i].to_netcdf(os.path.join(data_path+\"/historical-rad/\"+\"nbp_\"+item.esm+\"_\"+\"historical\"+\"_\"+realiz_sorted[i]+\"_eco2.nc\"))\n",
    "    ds_hist_nep_net[i].to_netcdf(os.path.join(data_path+\"/historical-rad/\"+\"nep_\"+item.esm+\"_\"+\"historical\"+\"_\"+realiz_sorted[i]+\"_net.nc\"))\n",
    "    ds_hist_nbp_net[i].to_netcdf(os.path.join(data_path+\"/historical-rad/\"+\"nbp_\"+item.esm+\"_\"+\"historical\"+\"_\"+realiz_sorted[i]+\"_net.nc\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the eCO2 effect estimated from 1pctCo2-bgc simulations from NEP and NBP in ssp585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ssp_nep_eco2 = []; ds_ssp_nbp_eco2 = []\n",
    "ds_ssp_nep_net = []; ds_ssp_nbp_net = []\n",
    "\n",
    "for i, item in enumerate(ds_ssp_nep):\n",
    "    esm = item.esm\n",
    "\n",
    "    for e, ee in enumerate(eco2_nep):\n",
    "        if eco2_nep[e].esm == esm:\n",
    "            content = ds_co2_new[i] * eco2_nep[e]\n",
    "            content1 = ds_co2_new[i] * eco2_nbp[e]\n",
    "            break\n",
    "        \n",
    "    ds_ssp_nep_eco2.append(content.assign_attrs(esm = item.esm))\n",
    "    ds_ssp_nbp_eco2.append(content1.assign_attrs(esm = item.esm))\n",
    "\n",
    "    net_nep = (ds_ssp_nep[i] - ds_ssp_nep_eco2[-1]); net_nep = net_nep.assign_attrs(esm = item.esm)\n",
    "    ds_ssp_nep_net.append(net_nep)\n",
    "\n",
    "    net_nbp = (ds_ssp_nbp[i] - ds_ssp_nbp_eco2[-1]); net_nbp = net_nbp.assign_attrs(esm = item.esm)\n",
    "    ds_ssp_nbp_net.append(net_nbp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Export files \n",
    "for i,item in enumerate(ds_ssp_nep_net):\n",
    "\n",
    "    ds_ssp_nep_eco2[i].to_netcdf(os.path.join(data_path+\"/ssp585-rad/\"+\"nep_\"+item.esm+\"_\"+\"ssp585\"+\"_\"+realiz_sorted[i]+\"_eco2.nc\"))\n",
    "    ds_ssp_nbp_eco2[i].to_netcdf(os.path.join(data_path+\"/ssp585-rad/\"+\"nbp_\"+item.esm+\"_\"+\"ssp585\"+\"_\"+realiz_sorted[i]+\"_eco2.nc\"))\n",
    "    ds_ssp_nep_net[i].to_netcdf(os.path.join(data_path+\"/ssp585-rad/\"+\"nep_\"+item.esm+\"_\"+\"ssp585\"+\"_\"+realiz_sorted[i]+\"_net.nc\"))\n",
    "    ds_ssp_nbp_net[i].to_netcdf(os.path.join(data_path+\"/ssp585-rad/\"+\"nbp_\"+item.esm+\"_\"+\"ssp585\"+\"_\"+realiz_sorted[i]+\"_net.nc\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_clim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
