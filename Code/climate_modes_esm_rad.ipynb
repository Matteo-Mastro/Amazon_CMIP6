{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = r'C:/Users/mastr/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Windows OS\n",
    "# os.environ['PROJ_LIB'] = r'/Users/mmastro/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Mac OS\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.mpl.ticker as cticker\n",
    "\n",
    "import geopandas as gpd\n",
    "from importlib.machinery import SourceFileLoader\n",
    "# imports the module from the given path\n",
    "gpd = SourceFileLoader(\"geopandas\",\"C:/Users/mastr/miniconda3/pkgs/geopandas-0.7.0-py_1/site-packages/geopandas/__init__.py\").load_module()\n",
    "from clisops.ops.subset import subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for moving average ##\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "## -- Function for get strings of dates in .nc input files -- ##\n",
    "def set_string_time(file, tname):\n",
    "    nctime = file.variables[tname][:] # get values\n",
    "    t_unit = file.variables[tname].units # get unit  \"days since 1950-01-01T00:00:00Z\"\n",
    "    t_cal = file.variables[tname].calendar\n",
    "    tvalue = nc.num2date(nctime,units = t_unit,calendar = t_cal)\n",
    "    str_time = [i.strftime(\"%Y-%m\") for i in tvalue]\n",
    "    str_timey = [i.strftime(\"%Y\") for i in tvalue]\n",
    "    str_timem = [i.strftime(\"%m\") for i in tvalue]\n",
    "    return [str_time, str_timey, str_timem]\n",
    "\n",
    "## Function for creating a path, if needed ##\n",
    "def checkDir(out_path):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "def cell_weight(ds):\n",
    "    R = 6.371e6\n",
    "    dϕ = np.deg2rad(ds.lat[1] - ds.lat[0])\n",
    "    dλ = np.deg2rad(ds.lon[1] - ds.lon[0])\n",
    "    dlat = R * dϕ * xr.ones_like(ds.lon)\n",
    "    dlon = R * dλ * np.cos(np.deg2rad(ds.lat))\n",
    "    cell_area = dlon * dlat\n",
    "    return(cell_area)\n",
    "\n",
    "def detrend_dim(da, dim, degree):\n",
    "    # detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=degree)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    da_det = (da - fit)\n",
    "    return da_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Path for netcdf files\n",
    "\n",
    "data_path = 'F:/Data/analysis/'               ## HDEXT\n",
    "# data_path = 'C:/Users/mastr/Documents/Amazon/ENSO'     # PC\n",
    "\n",
    "# -- Path for the output files (images, etc)\n",
    "# out_path = \"D:/Data/CMIP6/ENSO\"      ## HDEXT\n",
    "out_path = 'C:/Users/mastr/Documents/Amazon/ENSO_rad'\n",
    "\n",
    "# -- Create directories\n",
    "checkDir(data_path)\n",
    "checkDir(out_path)\n",
    "\n",
    "# esm = [\"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CESM2-WACCM\", \"CMCC-ESM2\", \"CNRM-ESM2-1\", \"CanESM5\",  \"E3SM-1-1-ECA\", \"IPSL-CM6A-LR\", \"MIROC-ES2L\", \"MPI-ESM1-2-LR\", \"NorESM2-MM\", \"TaiESM1\", \"UKESM1-0-LL\"]\n",
    "esm = [\"ACCESS-ESM1-5\", \"CanESM5\", \"CNRM-ESM2-1\", \"MIROC-ES2L\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"UKESM1-0-LL\"]\n",
    "esm = [\"MRI-ESM2-0\", \"NorESM2-LM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# for i, item in enumerate(DS_models):\n",
    "#     if item.lon[0] == 0.5:\n",
    "#         #DS_models[i] = DS_models[i].roll(lon = 180, roll_coords=False)                              # Roll by 180 steps to 0 360\n",
    "#         DS_models[i] = item.assign_coords(lon=(DS_models[i].lon - 0.5)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------- Open data (MODEL) ------- #####\n",
    "data_path = 'F:/Data/analysis/'              \n",
    "var_name = 'tos'\n",
    "scenario = 'historical'\n",
    "\n",
    "ds_hist = []\n",
    "for mm in esm:\n",
    "    filepath = glob.glob(os.path.join(data_path+scenario+\"/\"+var_name + '_'+ mm  + '_' + scenario + '_*_' + 'remap.nc'))[0]               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"], engine = 'netcdf4'); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    ds_hist.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "\n",
    "ds_ssp = []\n",
    "for mm in esm:\n",
    "    filepath = glob.glob(os.path.join(data_path+scenario+\"/\"+var_name + '_'+ mm  + '_' + scenario + '_*_' + 'remap.nc'))[0]               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"], engine ='netcdf4'); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    ds_ssp.append(content)\n",
    "\n",
    "# # Control for CALENDAR: convert cftime.datetime to np.datetime64\n",
    "for i, item in enumerate(ds_hist):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_hist[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "    \n",
    "# # Control for CALENDAR: convert cftime.datetime to np.datetime64\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        ds_ssp[i]['time'] = item.indexes['time'].to_datetimeindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_area_hist = [cell_weight(a) for a in ds_hist]\n",
    "cell_area_ssp = [cell_weight(a) for a in ds_ssp]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nino 3.4 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nino 3.4 with 5 month running mean\n",
    "\n",
    "nino34_hist = []\n",
    "for i, item in enumerate(ds_hist):\n",
    "    content = (item.sel(lat = slice(-5, 5), lon = slice(190, 240)).weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"]).groupby('time.month') - \n",
    "    item.sel(lat = slice(-5, 5), lon = slice(190, 240)).sel(time= slice(\"1981-01\", \"2010-12\")).groupby('time.month').mean('time').weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"])).compute(dim=var_name).rolling(time = 5, center = True).mean()\n",
    "    nino34_hist.append(content)\n",
    "\n",
    "nino34_ssp = []\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    content = (item.sel(lat = slice(-5, 5), lon = slice(190, 240)).weighted(cell_area_ssp[i]).mean([\"lat\", \"lon\"]).groupby('time.month') - \n",
    "    ds_hist[i].sel(lat = slice(-5, 5), lon = slice(190, 240)).sel(time= slice(\"1981-01\", \"2010-12\")).groupby('time.month').mean('time').weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"])).compute(dim=var_name).rolling(time = 5, center = True).mean()\n",
    "    nino34_ssp.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st order detrending (linear) \n",
    "\n",
    "# nino34_hist_det = []\n",
    "# for i, item in enumerate(nino34_hist):\n",
    "#     content = detrend_dim(nino34_hist[i][var_name], \"time\", 1)\n",
    "#     nino34_hist_det.append(content)\n",
    "\n",
    "# nino34_ssp_det = []\n",
    "# for i, item in enumerate(nino34_ssp):\n",
    "#     content = detrend_dim(nino34_ssp[i][var_name], \"time\", 1)\n",
    "#     nino34_ssp_det.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(12, 6)) #, constrained_layout=True)\n",
    "\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    axs.plot(nino34_hist[i]['time'],nino34_hist[i])\n",
    "    axs.plot(nino34_ssp[i]['time'],nino34_ssp[i])\n",
    "\n",
    "\n",
    "axs.set_title(\"NINO3.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Export files \n",
    "for i,item in enumerate(ds_ssp):\n",
    "    nino34_hist[i].to_netcdf(os.path.join(out_path+\"/nino34_\"+item.source_id+\"_\"+\"historical\"+\"_\"+item.variant_label+\"_trend.nc\"))\n",
    "    nino34_ssp[i].to_netcdf(os.path.join(out_path+\"/nino34_\"+item.source_id+\"_\"+\"ssp585\"+\"_\"+item.variant_label+\"_trend.nc\"))\n",
    " \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TNA index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TNA index\n",
    "\n",
    "tna_hist = []\n",
    "for i, item in enumerate(ds_hist):\n",
    "    content = (item.sel(lat = slice(5,25), lon = slice(305,345)).weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"]).groupby('time.month') - \n",
    "    item.sel(lat = slice(5,25), lon = slice(305,345)).sel(time= slice(\"1981-01\", \"2010-12\")).groupby('time.month').mean('time').weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"])).compute(dim=var_name).rolling(time = 5, center = True).mean()\n",
    "    tna_hist.append(content)\n",
    "\n",
    "tna_ssp = []\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    content = (item.sel(lat = slice(5,25), lon = slice(305,345)).weighted(cell_area_ssp[i]).mean([\"lat\", \"lon\"]).groupby('time.month') - \n",
    "    ds_hist[i].sel(lat = slice(5,25), lon = slice(305,345)).sel(time= slice(\"1981-01\", \"2010-12\")).groupby('time.month').mean('time').weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"])).compute(dim=var_name).rolling(time = 5, center = True).mean()\n",
    "    tna_ssp.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st order detrending (linear) \n",
    "\n",
    "tna_hist_det = []\n",
    "for i, item in enumerate(tna_hist):\n",
    "    content = detrend_dim(tna_hist[i][var_name], \"time\", 3)\n",
    "    tna_hist_det.append(content)\n",
    "\n",
    "tna_ssp_det = []\n",
    "for i, item in enumerate(tna_ssp):\n",
    "    content = detrend_dim(tna_ssp[i][var_name], \"time\", 3)\n",
    "    tna_ssp_det.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(12, 6)) #, constrained_layout=True)\n",
    "\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    axs.plot(tna_hist_det[i]['time'],tna_hist_det[i])\n",
    "    axs.plot(tna_ssp_det[i]['time'],tna_ssp_det[i])\n",
    "\n",
    "\n",
    "axs.set_title(\"TNA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export files \n",
    "for i,item in enumerate(ds_ssp):\n",
    "    tna_hist_det[i].to_netcdf(os.path.join(out_path+\"/tna_\"+item.source_id+\"_\"+\"historical\"+\"_\"+item.variant_label+\".nc\"))\n",
    "    tna_ssp_det[i].to_netcdf(os.path.join(out_path+\"/tna_\"+item.source_id+\"_\"+\"ssp585\"+\"_\"+item.variant_label+\".nc\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSA index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSA with subset from clisops\n",
    "\n",
    "tsa_hist = []\n",
    "for i, item in enumerate(ds_hist):\n",
    "    content = (subset(ds=item, area=(-30, -20, 10, 0), output_type=\"xarray\")[0].weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"]).groupby('time.month') - \n",
    "    subset(ds=item, area=(-30, -20, 10, 0), output_type=\"xarray\")[0].sel(time= slice(\"1981-01\", \"2010-12\")).groupby('time.month').mean('time').weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"])).compute(dim=var_name)\n",
    "    tsa_hist.append(content)\n",
    "\n",
    "tsa_ssp = []\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    content = (subset(ds=item, area=(-30, -20, 10, 0), output_type=\"xarray\")[0].weighted(cell_area_ssp[i]).mean([\"lat\", \"lon\"]).groupby('time.month') - \n",
    "    subset(ds=ds_hist[i], area=(-30, -20, 10, 0), output_type=\"xarray\")[0].sel(time= slice(\"1981-01\", \"2010-12\")).groupby('time.month').mean('time').weighted(cell_area_hist[i]).mean([\"lat\", \"lon\"])).compute(dim=var_name)\n",
    "    tsa_ssp.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st order detrending (linear) \n",
    "\n",
    "tsa_hist_det = []\n",
    "for i, item in enumerate(tsa_hist):\n",
    "    content = detrend_dim(tsa_hist[i][var_name], \"time\", 3)\n",
    "    tsa_hist_det.append(content)\n",
    "\n",
    "tsa_ssp_det = []\n",
    "for i, item in enumerate(tsa_ssp):\n",
    "    content = detrend_dim(tsa_ssp[i][var_name], \"time\", 3)\n",
    "    tsa_ssp_det.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(12, 6)) #, constrained_layout=True)\n",
    "\n",
    "for i, item in enumerate(ds_ssp):\n",
    "    axs.plot(tsa_hist_det[i]['time'],tsa_hist_det[i])\n",
    "    axs.plot(tsa_ssp_det[i]['time'],tsa_ssp_det[i])\n",
    "\n",
    "\n",
    "axs.set_title(\"TSA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Export files \n",
    "for i,item in enumerate(ds_ssp):\n",
    "    tsa_hist_det[i].to_netcdf(os.path.join(out_path+\"/tsa_\"+item.source_id+\"_\"+\"historical\"+\"_\"+item.variant_label+\".nc\"))\n",
    "    tsa_ssp_det[i].to_netcdf(os.path.join(out_path+\"/tsa_\"+item.source_id+\"_\"+\"ssp585\"+\"_\"+item.variant_label+\".nc\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('clim_cartopy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "095e6f016c335eb5cb9e8bbcd74bb0970d058777132f694436b810215233da1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
