{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = r'C:/Users/mastr/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Windows OS\n",
    "# os.environ['PROJ_LIB'] = r'/Users/mmastro/miniconda3/pkgs/proj4-5.2.0-ha925a31_1/Library/share'     ## Mac OS\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#from scipy.signal import argrelextrema                      # Find local Maxima-Minima in numpy array\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for getting the indeces of the array *arr* between the *var_min* and *var_max* values:\n",
    "def getRangeIndexes(arr, var_min, var_max):\n",
    "    return np.where((np.array(arr) >= var_min) & (np.array(arr) <= var_max))[0]\n",
    "\n",
    "## Function for getting the index of the array for the value ##\n",
    "def getIndexes(arr, value):\n",
    "    return np.where(arr = value)\n",
    "\n",
    "## Function for moving average ##\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "## Fuction for subsetting colormap values ## \n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "## -- Function for get strings of dates in .nc input files -- ##\n",
    "def set_string_time(file, tname):\n",
    "    nctime = file.variables[tname][:] # get values\n",
    "    t_unit = file.variables[tname].units # get unit  \"days since 1950-01-01T00:00:00Z\"\n",
    "    t_cal = file.variables[tname].calendar\n",
    "    tvalue = nc.num2date(nctime,units = t_unit,calendar = t_cal)\n",
    "    str_time = [i.strftime(\"%Y-%m\") for i in tvalue]\n",
    "    str_timey = [i.strftime(\"%Y\") for i in tvalue]\n",
    "    str_timem = [i.strftime(\"%m\") for i in tvalue]\n",
    "    return [str_time, str_timey, str_timem]\n",
    "\n",
    "## Function for creating a path, if needed ##\n",
    "def checkDir(out_path):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open SST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------- Open data (MODEL) ------- #####\n",
    "data_path = 'C:/Users/mastr/Documents/Amazon'\n",
    "scenario = 'historical'\n",
    "files = \"nino34\" + '_*_' + scenario + '_*'\n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/ENSO/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)   \n",
    "\n",
    "nino34_hist = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/ENSO/'+files))):               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"]); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    nino34_hist.append(content)\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = \"nino34\" + '_*_' + scenario + '_*' \n",
    "\n",
    "nino34_ssp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/ENSO/'+files))):               # sorted is case sensitive                             ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"]); #content = content.mean(dim=\"time\")         ## values   var     dims    coords\n",
    "    nino34_ssp.append(content)\n",
    "\n",
    "# Standardize calendar \n",
    "# for i, item in enumerate(nino34_hist):\n",
    "#     if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "#         nino34_hist[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "    \n",
    "# for i, item in enumerate(nino34_ssp):\n",
    "#     if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "#         nino34_ssp[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "## Normalization\n",
    "for i,item in enumerate(nino34_hist):\n",
    "    nino34_hist[i] =  ((nino34_hist[i] - (nino34_hist[i].mean(dim='time'))).compute()/(nino34_hist[i].std(dim='time'))).compute()\n",
    "\n",
    "for i,item in enumerate(nino34_ssp):\n",
    "    nino34_ssp[i] =  ((nino34_ssp[i] - (nino34_ssp[i].mean(dim='time'))).compute()/(nino34_ssp[i].std(dim='time'))).compute()\n",
    "\n",
    "\n",
    "## Resample from Monthly to seasonal timesteps\n",
    "nino34_hist_seas = []\n",
    "for i,item in enumerate(nino34_hist):\n",
    "    content = item.resample(time=\"QS\", label='left').mean()\n",
    "    nino34_hist_seas.append(content)\n",
    "\n",
    "nino34_ssp_seas = []\n",
    "for i,item in enumerate(nino34_ssp):\n",
    "    content = item.resample(time=\"QS\", label='left').mean()\n",
    "    nino34_ssp_seas.append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open LAND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------- Open MODEL data HISTORICAL (ENSMEAN) ------- #####\n",
    "data_path = 'E:/Data/analysis/'              \n",
    "\n",
    "scenario = 'historical'\n",
    "var_name = 'nep'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "\n",
    "files_list = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):               # List of files sorted by name\n",
    "        content = nc.Dataset(filepath)\n",
    "        files_list.append(content)                                              # to retrieve netcdf original ATTRIBUTES\n",
    "\n",
    "DS_models_hist = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])          ## values   var     dims    coords\n",
    "    DS_models_hist.append(content)\n",
    "\n",
    "##### ------- UNIFORM CALENDAR ------- #####\n",
    "for i, item in enumerate(DS_models_hist):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        DS_models_hist[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# -- Resample from Months to Seasons\n",
    "DS_models_hist_seas = []\n",
    "for i,item in enumerate(DS_models_hist):\n",
    "    content = item.resample(time=\"QS\", label='left').mean()\n",
    "    DS_models_hist_seas.append(content)\n",
    "\n",
    "##### ============================================= #####\n",
    "\n",
    "##### ------- Open MODEL data SSP (ENSMEAN) ------- #####\n",
    "\n",
    "scenario = 'ssp585'\n",
    "files = var_name + '_*_' + scenario + '_*' \n",
    "\n",
    "DS_models_ssp = []\n",
    "for filepath in sorted(glob.glob(os.path.join(data_path+'/'+scenario+'/'+files))):                                       ## List of files sorted by name\n",
    "    content = xr.open_dataarray(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])           ## values   var     dims    coords\n",
    "    DS_models_ssp.append(content)                                                                 \n",
    "\n",
    "##### ------- UNIFORM CALENDAR ------- #####\n",
    "for i, item in enumerate(DS_models_ssp):\n",
    "    if item['time'].dt.calendar == 'noleap' or item['time'].dt.calendar == '360_day':\n",
    "        DS_models_ssp[i]['time'] = item.indexes['time'].to_datetimeindex()\n",
    "\n",
    "# -- Resample from Months to Seasons\n",
    "DS_models_ssp_seas = []\n",
    "for i,item in enumerate(DS_models_ssp):\n",
    "    content = item.resample(time=\"QS\", label='left').mean()\n",
    "    DS_models_ssp_seas.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm = []\n",
    "for filepath in sorted(glob.glob(os.path.join('E:/Data/analysis/'+'/'+'historical'+'/'+'nep' + '_*_' + 'historical' + '_*'))):     \n",
    "        content = nc.Dataset(filepath).source_id\n",
    "        esm.append(content)   \n",
    "\n",
    "for i,model in enumerate(esm):\n",
    "        if model == \"CESM2-WACCM\":\n",
    "                DS_models_hist_seas[i] = DS_models_hist_seas[i]*-1\n",
    "                DS_models_ssp_seas[i] = DS_models_ssp_seas[i]*-1\n",
    "        else:\n",
    "                None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composites analysis\n",
    "\n",
    "Define Nino events as those DJF seasons lying above 90th perc\\\n",
    "Define Nina events as those DJF seasons lying below 10th perc\\\n",
    "Define Mean State conditions as those DJF seasons in between\n",
    "\n",
    "Percentiles calculated in the historical scenario (1901-1960) and in the ssp scenario (2040-2100) so that to have the same number of events\\\n",
    "\n",
    "Historical and Future Anomalies are computed with respect to the 1901-1960 and 2041-2100 climatologies respectively (these calculated excluding ENSO events of high magnitude (Nino and Nina events)). \\\n",
    "ENSO effect is computed considering the difference between the Future DJF anomalies with respect to the Historical DJF anomalies.\\\n",
    "MEAN STATE CHANGE is computed considering the difference between Future NEUTRAL conditions and Historical NEUTRAL conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seas = \"DJF\"\n",
    "\n",
    "def composite_analysis(ds, ds_sst, quant1, quant2, time_init, time_end):\n",
    "    perc90 = ds_sst.sel(time = ds_sst.time.dt.season==seas).sel(time = slice(time_init,time_end)).quantile(quant1, dim = 'time')      # Percentile only from the DJF season\n",
    "    perc10 = ds_sst.sel(time = ds_sst.time.dt.season==seas).sel(time = slice(time_init,time_end)).quantile(quant2, dim = 'time')\n",
    "    nino_events = ds.sel(time = ds.time.dt.season==seas).sel(time = slice(time_init,time_end)).where(ds_sst > perc90, drop=True)\n",
    "    nina_events = ds.sel(time = ds.time.dt.season==seas).sel(time = slice(time_init,time_end)).where(ds_sst < perc10, drop=True)\n",
    "    ds_nino = ds.sel(time = nino_events.time, method =\"nearest\").sel(time = slice(time_init,time_end))  \n",
    "    ds_nina = ds.sel(time = nina_events.time, method =\"nearest\").sel(time = slice(time_init,time_end))          \n",
    "    # ds_nino = ds_nino.mean(dim = \"time\")        # Composites of El Nino\n",
    "    # ds_nina = ds_nina.mean(dim = \"time\")        # Composites of La Nina\n",
    "    \n",
    "    content = ds.sel(time = ds.time.dt.season==seas).sel(time=slice(time_init,time_end))\n",
    "    content1 = nino_events.groupby(nino_events.time.dt.year).mean(\"time\")  # NINO events between 1901 an6 1910\n",
    "    content2 = nina_events.groupby(nina_events.time.dt.year).mean(\"time\")  # NINO events between 1901 an6 1910\n",
    "    for ninoyrs in np.array(content1.year):                                                                            # drop NINO events'years recursively\n",
    "        content = content.where(content.time.dt.year != ninoyrs, drop = True)\n",
    "    for ninayrs in np.array(content2.year):\n",
    "        content = content.where(content.time.dt.year != ninayrs, drop = True)                                          # drop NINO events'years recursively   \n",
    "    ds_clim_neutral = content  # Composites of mean state changes (Neutral years)\n",
    "\n",
    "    return ds_clim_neutral, ds_nino, ds_nina\n",
    "\n",
    "results_hist = [composite_analysis(a, b, 0.9, 0.1, \"1901\", \"1960\") for a,b in zip(DS_models_hist_seas, nino34_hist_seas)]\n",
    "ds_clim_neutral_hist_nep = [result[0] for result in results_hist]\n",
    "ds_nino_hist_nep = [result[1] for result in results_hist]\n",
    "ds_nina_hist_nep = [result[2] for result in results_hist]\n",
    "\n",
    "results_ssp = [composite_analysis(a, b, 0.9, 0.1, \"2041\", \"2100\") for a,b in zip(DS_models_ssp_seas, nino34_ssp_seas)]\n",
    "ds_clim_neutral_ssp_nep = [result[0] for result in results_ssp]\n",
    "ds_nino_ssp_nep = [result[1] for result in results_ssp]\n",
    "ds_nina_ssp_nep = [result[2] for result in results_ssp]\n",
    "\n",
    "# For MannWhitneyU calculation, the significance is calculated among a composite for SSP (eg (time=12,lon=30,lat=37)) and HIST (eg (time=5,lon=30,lat=37))\n",
    "# For Anomalies calculation , the anomalies are calculated among the average of those SSP and HIST samples.\n",
    "# As a consequence, in the code above we need to take the mean along time dimension of the followings:\n",
    "##  DS_models_ssp_seas_clim_allseason\n",
    "##  DS_models_ssp_seas_clim_nonino\n",
    "##  DS_models_ssp_seas_clim_nino\n",
    "\n",
    "## -- NINO HISTORICAL ANOMALIES\n",
    "nino_anom_hist = [(a - b).compute(dim=var_name) for a,b in zip(ds_nino_hist_nep, ds_clim_neutral_hist_nep)]\n",
    "\n",
    "## -- NINO SSP585 ANOMALIES\n",
    "nino_anom_ssp = [(a - b).compute(dim=var_name) for a,b in zip(ds_nino_ssp_nep, ds_clim_neutral_ssp_nep )]\n",
    "\n",
    "## -- NINO EFFECT (considering the difference between anomalies)\n",
    "nino_effect = [(a - b).compute(dim=var_name) for a,b in zip(nino_anom_ssp, nino_anom_hist)]\n",
    "\n",
    "## -- NINA HISTORICAL ANOMALIES\n",
    "nina_anom_hist = [(a - b).compute(dim=var_name) for a,b in zip(ds_nina_hist_nep, ds_clim_neutral_hist_nep)]\n",
    "\n",
    "## -- NINA SSP585 ANOMALIES\n",
    "nina_anom_ssp = [(a - b).compute(dim=var_name) for a,b in zip(ds_nina_ssp_nep, ds_clim_neutral_ssp_nep)]\n",
    "\n",
    "## -- NINA EFFECT (considering the difference between anomalies)\n",
    "nina_effect = [(a - b).compute(dim=var_name) for a,b in zip(nina_anom_ssp, nina_anom_hist)]\n",
    "\n",
    "## -- MEAN STATE CHANGE\n",
    "msc = [(a - b).compute(dim=var_name) for a,b in zip(ds_clim_neutral_ssp_nep, ds_clim_neutral_hist_nep)]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Test (Mann-Whitney)\n",
    "### Control for False Discovery Rate (FDR) in multiple testing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### --------- Mann-Whitney Test --------- ####\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Test applied on a grid-cell basis. For every gid-cell, the statistical difference between two time series is computed.\n",
    "# 1st time series: Nino years' (DS_models_hist_nino)\n",
    "# 2nd time series: Reference climatology (DS_models_hist_clim or DS_models_hist_clim_neutral)  \n",
    "\n",
    "## -- Function for grid-cell operations -- ##\n",
    "def multi_apply_along_axis(func1d, axis, arrs, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Given a function `func1d(A, B, C, ..., *args, **kwargs)`  that acts on \n",
    "    multiple one dimensional arrays, apply that function to the N-dimensional\n",
    "    arrays listed by `arrs` along axis `axis`\n",
    "    \n",
    "    If `arrs` are one dimensional this is equivalent to::\n",
    "    \n",
    "        func1d(*arrs, *args, **kwargs)\n",
    "    \n",
    "    If there is only one array in `arrs` this is equivalent to::\n",
    "    \n",
    "        numpy.apply_along_axis(func1d, axis, arrs[0], *args, **kwargs)\n",
    "        \n",
    "    All arrays in `arrs` must have compatible dimensions to be able to run\n",
    "    `numpy.concatenate(arrs, axis)`\n",
    "    \n",
    "    Arguments:\n",
    "        func1d:   Function that operates on `len(arrs)` 1 dimensional arrays,\n",
    "                  with signature `f(*arrs, *args, **kwargs)`\n",
    "        axis:     Axis of all `arrs` to apply the function along\n",
    "        arrs:     Iterable of numpy arrays\n",
    "        *args:    Passed to func1d after array arguments\n",
    "        **kwargs: Passed to func1d as keyword arguments\n",
    "    \"\"\"\n",
    "    # Concatenate the input arrays along the calculation axis to make one big\n",
    "    # array that can be passed in to `apply_along_axis`\n",
    "    carrs = np.concatenate(arrs, axis)\n",
    "    \n",
    "    # We'll need to split the concatenated arrays up before we apply `func1d`,\n",
    "    # here's the offsets to split them back into the originals\n",
    "    offsets=[]\n",
    "    start=0\n",
    "    for i in range(len(arrs)-1):\n",
    "        start += arrs[i].shape[axis]\n",
    "        offsets.append(start)\n",
    "            \n",
    "    # The helper closure splits up the concatenated array back into the components of `arrs`\n",
    "    # and then runs `func1d` on them\n",
    "    def helperfunc(a, *args, **kwargs):\n",
    "        arrs = np.split(a, offsets)\n",
    "        return func1d(*[*arrs, *args], **kwargs)\n",
    "    \n",
    "    # Run `apply_along_axis` along the concatenated array\n",
    "    return np.apply_along_axis(helperfunc, axis, carrs, *args, **kwargs)\n",
    "\n",
    "\n",
    "def xr_multipletest(p, alpha=0.05, method='fdr_bh', **multipletests_kwargs):\n",
    "    \"\"\"Apply statsmodels.stats.multitest.multipletests for multi-dimensional xr.objects.\"\"\"\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    # stack all to 1d array\n",
    "    p_stacked = p.stack(s=p.dims)\n",
    "    # mask only where not nan: https://github.com/statsmodels/statsmodels/issues/2899\n",
    "    mask = np.isfinite(p_stacked)\n",
    "    pvals_corrected = np.full(p_stacked.shape, np.nan)\n",
    "    reject = np.full(p_stacked.shape, np.nan)\n",
    "    # apply test where mask\n",
    "    reject[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[0]\n",
    "    pvals_corrected[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[1]\n",
    "\n",
    "    def unstack(reject, p_stacked):\n",
    "        \"\"\"Exchange values from p_stacked with reject (1d array) and unstack.\"\"\"\n",
    "        xreject = p_stacked.copy()\n",
    "        xreject.values = reject\n",
    "        xreject = xreject.unstack()\n",
    "        return xreject\n",
    "\n",
    "    reject = unstack(reject, p_stacked)\n",
    "    pvals_corrected = unstack(pvals_corrected, p_stacked)\n",
    "    return reject, pvals_corrected\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MW test with FDR correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----- MW Test on every MODEL with FDR correction ----- ##\n",
    "# Obtain dim: nmodels*2*(lon*lat)\n",
    "# DS_models_hist_seas_nino & DS_models_hist_seas_clim_neutral ---> MannWhitney_historical\n",
    "# DS_models_ssp_seas_nino & DS_models_ssp_seas_clim_neutral ---> MannWhitney_ssp\n",
    "# DS_models_ssp_seas_clim_neutral & DS_models_hist_seas_clim_neutral ---> MannWhitney_ssp_msc\n",
    "# DS_models_ssp_seas_nino & DS_models_hist_seas_nino ---> MannWhitney_ssp_nino\n",
    "\n",
    "mannwhitney_msc=[]\n",
    "mannwhitney_msc_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_clim_neutral_ssp_nep[i].fillna(0), ds_clim_neutral_hist_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_clim_neutral_ssp_nep[i].lon.values), lat= (ds_clim_neutral_hist_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_msc.append(mw)\n",
    "    mannwhitney_msc_fdr.append(mw_fdr)\n",
    "\n",
    "mannwhitney_nino_effect=[]\n",
    "mannwhitney_nino_effect_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_nino_ssp_nep[i].fillna(0), ds_nino_hist_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_nino_ssp_nep[i].lon.values), lat= (ds_nino_hist_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_nino_effect.append(mw)\n",
    "    mannwhitney_nino_effect_fdr.append(mw_fdr)\n",
    "\n",
    "mannwhitney_nina_effect=[]\n",
    "mannwhitney_nina_effect_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_nina_ssp_nep[i].fillna(0), ds_nina_hist_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_nina_ssp_nep[i].lon.values), lat= (ds_nina_hist_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_nina_effect.append(mw)\n",
    "    mannwhitney_nina_effect_fdr.append(mw_fdr)\n",
    "\n",
    "mannwhitney_nino_anom_hist=[]\n",
    "mannwhitney_nino_anom_hist_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_nino_hist_nep[i].fillna(0), ds_clim_neutral_hist_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_nino_hist_nep[i].lon.values), lat= (ds_clim_neutral_hist_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_nino_anom_hist.append(mw)\n",
    "    mannwhitney_nino_anom_hist_fdr.append(mw_fdr)\n",
    "\n",
    "mannwhitney_nina_anom_hist=[]\n",
    "mannwhitney_nina_anom_hist_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_nina_hist_nep[i].fillna(0), ds_clim_neutral_hist_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_nina_hist_nep[i].lon.values), lat= (ds_clim_neutral_hist_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_nina_anom_hist.append(mw)\n",
    "    mannwhitney_nina_anom_hist_fdr.append(mw_fdr)\n",
    "\n",
    "mannwhitney_nino_anom_ssp=[]\n",
    "mannwhitney_nino_anom_ssp_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_nino_ssp_nep[i].fillna(0), ds_clim_neutral_ssp_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_nino_ssp_nep[i].lon.values), lat= (ds_clim_neutral_ssp_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_nino_anom_ssp.append(mw)\n",
    "    mannwhitney_nino_anom_ssp_fdr.append(mw_fdr)\n",
    "\n",
    "mannwhitney_nina_anom_ssp=[]\n",
    "mannwhitney_nina_anom_ssp_fdr=[]\n",
    "for i,item in enumerate(files_list):\n",
    "    mw = multi_apply_along_axis(mannwhitneyu, 0, [ds_nina_ssp_nep[i].fillna(0), ds_clim_neutral_ssp_nep[i].fillna(0)])[1]\n",
    "    mw_xr = xr.DataArray(mw, dims=[\"lat\",\"lon\"], coords=dict(lon = (ds_nina_ssp_nep[i].lon.values), lat= (ds_clim_neutral_ssp_nep[i].lat.values)))\n",
    "    mw_fdr = xr_multipletest(mw_xr)\n",
    "    mannwhitney_nina_anom_ssp.append(mw)\n",
    "    mannwhitney_nina_anom_ssp_fdr.append(mw_fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Save and export Mann-Whitney list data\n",
    "import pickle\n",
    "\n",
    "data_path = 'C:/Users/mastr/Documents/Amazon'\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"msc\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_msc, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nino_effect\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nino_effect, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nina_effect\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nina_effect, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nino_anom_hist\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nino_anom_hist, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nina_anom_hist\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nina_anom_hist, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nino_anom_ssp\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nino_anom_ssp, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nina_anom_ssp\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nina_anom_ssp, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Save and export Mann-Whitney list data\n",
    "import pickle\n",
    "\n",
    "data_path = 'C:/Users/mastr/Documents/Amazon'\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"msc_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_msc_fdr, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nino_effect_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nino_effect_fdr, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nina_effect_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nina_effect_fdr, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nino_anom_hist_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nino_anom_hist_fdr, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nina_anom_hist_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nina_anom_hist_fdr, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nino_anom_ssp_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nino_anom_ssp_fdr, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"/analysis/MannWhitney_\"+var_name+\"_\"+\"nina_anom_ssp_fdr\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mannwhitney_nina_anom_ssp_fdr, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('clim_cartopy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "095e6f016c335eb5cb9e8bbcd74bb0970d058777132f694436b810215233da1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
